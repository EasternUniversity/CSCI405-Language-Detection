{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd55be",
   "metadata": {},
   "source": [
    "# Understanding the Kaggle Data\n",
    "The dataset acquired from [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/data) contains words from several different languages. The noise contained in the dataset are duplicate words. To reduce this noise, the words will be broken down into single and double characters, then rated based on how often they show up in that respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data*\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18753df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "language_list = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017c263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "                               ...                        \n",
       "11964    باباجان غفورف تاریخ‌دان و نویسندهٔ کتاب تاریخ ...\n",
       "21575    en  fue invitado por fernando ii para ocupar l...\n",
       "5390     doğu kanada atabasklarına geleneksel olarak dü...\n",
       "860      پژواک د يوې ځانگړې پروژې په توگه د اساسي قانون...\n",
       "15795    テンサイについては糖分を高度に精製する必要があることからサトウキビと同じような黒糖を作るのは...\n",
       "Name: Text, Length: 17600, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "            \n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679218c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_2(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['english', 'estonian', 'swedish', 'thai', 'tamil', 'dutch', 'japanese', 'turkish', 'latin', 'urdu',\n",
    "             'indonesian', 'portuguese', 'french', 'chinese', 'korean', 'hindi', 'spanish', 'pushto', 'persian',\n",
    "             'romanian', 'russian', 'arabic']\n",
    "    \n",
    "    data_frame = pd.DataFrame(new_arr, columns = names)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]\n",
    "\n",
    "panda = X_train.head()\n",
    "some_data = feature_engineering_2(panda, new_chars)\n",
    "some_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ed57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = X_train.head(6000)\n",
    "X_train_2_eng = feature_engineering_2(X_train_2, new_chars)\n",
    "\n",
    "y_train_2 = y_train.head(6000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf_2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf_2.fit(X_train_2_eng, y_train_2)\n",
    "\n",
    "X_test_2 = feature_engineering_2(X_test, new_chars)\n",
    "y_preds_2 = rnd_clf_2.predict(X_test_2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score_2 = accuracy_score(y_test, y_preds_2)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa73b2",
   "metadata": {},
   "source": [
    "### First Test Model\n",
    "----\n",
    "This is a test model to experiment how to implement the finalized model onto the flask website (via pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9187aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters for first model\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "X_train_2 = X_train.head(3000)\n",
    "model_one = feature_engineering(X_train_2, chars_2)\n",
    "model_one\n",
    "\n",
    "y_train_one = y_train.head(3000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(model_one, y_train_one)\n",
    "\n",
    "X_test_1 = feature_engineering(X_test, chars_2)\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))\n",
    "\n",
    "import pickle #ask about pickle\n",
    "\n",
    "saved_model = pickle.dumps(rnd_clf)\n",
    "\n",
    "rdf_from_pickle = pickle.loads(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5739d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = X_train.head()\n",
    "panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce97ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_22_features = ['e', 't', 'a', 'i', 'o', 'á', 'é', 'í']\n",
    "\n",
    "test_data = feature_engineering(panda, first_22_features)\n",
    "test_data_2 = numpy_into_dataframe(test_data, first_22_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa4cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>t</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>o</th>\n",
       "      <th>á</th>\n",
       "      <th>é</th>\n",
       "      <th>í</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          e         t         a         i         o    á    é    í\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0\n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0\n",
       "2  0.106299  0.051181  0.086614  0.051181  0.023622  0.0  0.0  0.0\n",
       "3  0.000000  0.006920  0.006920  0.006920  0.003460  0.0  0.0  0.0\n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2a28",
   "metadata": {},
   "source": [
    "## Creating Features: Most Used Characters\n",
    "To train the models to determine which language is being used by the user, we first need to know which characters are used in each language. The best approach for this is to used the dataset, _which is already using the characters from each language_, and find the **most used characters** in them.\n",
    "\n",
    "### Kaggle Resources\n",
    "----\n",
    "The following code is taken from the [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/notebook) to understand how to process characters for each language. Kaggle uses the `CountVectorizor` Method from the `sklearn` module to tokenize the characters into readable 1's and 0's. Then, it counts how many times that character has been used in the sample data provided.\n",
    "\n",
    "This method reduces the necessity of locating alphabets for each language and creating custom functions to find the most used characters in the dataset.\n",
    " \n",
    "**To summarize Kaggle's findings: Languages based off the Latin Alphabet are easier to differentiate from each other in the data set while Languages with their own Alphabet, _like Chinese and Japanese_, can be differentiated by single characters alone.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3797753",
   "metadata": {},
   "source": [
    "This code here fits and transforms the X_train and X_test data sets into readable 1's and 0's and counts the number of times a specific character shows up in the datasets.The `min_df` parameter tells `CountVectorizor` to save any characters that are used **at least 1% of the time** in the dataset.\n",
    "\n",
    "These matrices are saved as `X_top1Percent_train_raw` and `X_top1Percent_test_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb378224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a list of single and double characters from the top 1% of to be used as features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b0c14",
   "metadata": {},
   "source": [
    "The `train_lang_dict()` function takes in the raw vectorized X_train and y_train data sets and converts them into readable dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a14148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Command from Kaggle connects the character features to their specific language\n",
    "\n",
    "# Aggregate Unigrams per language\n",
    "def train_lang_dict(X_raw_counts, y_train):\n",
    "    lang_dict = {}\n",
    "    for i in range(len(y_train)):\n",
    "        lang = y_train[i]\n",
    "        v = np.array(X_raw_counts[i])\n",
    "        if not lang in lang_dict:\n",
    "            lang_dict[lang] = v\n",
    "        else:\n",
    "            lang_dict[lang] += v\n",
    "            \n",
    "    # to relative\n",
    "    for lang in lang_dict:\n",
    "        v = lang_dict[lang]\n",
    "        lang_dict[lang] = v / np.sum(v)\n",
    "        \n",
    "    return lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26b5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2, max_df=.9)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62b41a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>（</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>）。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>：</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3078 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      \"\n",
       "1      -\n",
       "2      [\n",
       "3      a\n",
       "4      b\n",
       "...   ..\n",
       "3073   （\n",
       "3074   ）\n",
       "3075  ）。\n",
       "3076   ，\n",
       "3077   ：\n",
       "\n",
       "[3078 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n",
    "\n",
    "top1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names_out()\n",
    "\n",
    "pd.DataFrame(top1PercentFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f036f3",
   "metadata": {},
   "source": [
    "The `getRelevantGramsPerLanguage()` function processes the dictionary and returns a dictionary with only the top 50 **most used** characters for **each** language. This number _can_ be changed by setting `top=x` when you call `getRelevantGramsPerLanguage()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e18e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantGramsPerLanguage(features, language_dict, top=50):\n",
    "    relevantGramsPerLanguage = {}\n",
    "    for lang in language_list:\n",
    "        chars = []\n",
    "        relevantGramsPerLanguage[lang] = chars\n",
    "        v = language_dict[lang]\n",
    "        sortIndex = (-v).argsort()[:top]\n",
    "        for i in range(len(sortIndex)):\n",
    "            chars.append(features[sortIndex[i]])\n",
    "    return relevantGramsPerLanguage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae2d19",
   "metadata": {},
   "source": [
    "Below Displays the top 8 and 10 Characters from each language in a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a571530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Urdu': ['ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن', 'ہ'],\n",
       " 'French': ['e', 'a', 'n', 's', 'i', 'r', 't', 'l'],\n",
       " 'Pushto': ['و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر', 'ل'],\n",
       " 'Spanish': ['e', 'a', 'o', 'n', 's', 'r', 'i', 'l'],\n",
       " 'Romanian': ['e', 'a', 'i', 'r', 'n', 't', 'u', 'l'],\n",
       " 'Estonian': ['a', 'i', 'e', 's', 't', 'l', 'n', 'u'],\n",
       " 'Indonesian': ['a', 'n', 'e', 'i', 'r', 'u', 't', 's'],\n",
       " 'Persian': ['ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و', 'م'],\n",
       " 'Dutch': ['e', 'n', 'a', 'i', 'r', 't', 'o', 'd'],\n",
       " 'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と'],\n",
       " 'Korean': ['이', '의', '다', '의 ', '에', '는', '는 ', '하'],\n",
       " 'Tamil': ['்', 'க', 'ு', 'ி', 'த', '் ', 'ப', 'ம'],\n",
       " 'English': ['e', 'a', 't', 'i', 'o', 'n', 's', 'r'],\n",
       " 'Swedish': ['e', 'r', 'a', 'n', 't', 'i', 's', 'd'],\n",
       " 'Thai': ['า', 'น', 'ร', 'ก', 'อ', '่', 'เ', 'ง'],\n",
       " 'Turkish': ['a', 'e', 'i', 'n', 'r', 'l', 'ı', 'd'],\n",
       " 'Portugese': ['a', 'e', 'o', 's', 'i', 'r', 'd', 'n'],\n",
       " 'Arabic': ['ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا', 'ن'],\n",
       " 'Chinese': ['，', '的', '。', '年', '在', '、', '一', '中'],\n",
       " 'Hindi': ['ा', 'क', 'र', '्', 'े', 'ि', 'स', ' क'],\n",
       " 'Russian': ['о', 'и', 'е', 'а', 'н', 'с', 'р', 'т'],\n",
       " 'Latin': ['i', 'a', 'e', 't', 's', 'n', 'r', 'u']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top8PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=8)\n",
    "top8PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d01a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Urdu': ['ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن', 'ہ', 'ے', 'ل'],\n",
       " 'French': ['e', 'a', 'n', 's', 'i', 'r', 't', 'l', 'e ', 'u'],\n",
       " 'Pushto': ['و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر', 'ل', 'ن', ' د'],\n",
       " 'Spanish': ['e', 'a', 'o', 'n', 's', 'r', 'i', 'l', 'd', 't'],\n",
       " 'Romanian': ['e', 'a', 'i', 'r', 'n', 't', 'u', 'l', 'o', 'c'],\n",
       " 'Estonian': ['a', 'i', 'e', 's', 't', 'l', 'n', 'u', 'o', 'k'],\n",
       " 'Indonesian': ['a', 'n', 'e', 'i', 'r', 'u', 't', 's', 'an', 'k'],\n",
       " 'Persian': ['ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و', 'م', 'ت', 'ب'],\n",
       " 'Dutch': ['e', 'n', 'a', 'i', 'r', 't', 'o', 'd', 's', 'n '],\n",
       " 'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と', 'ー', 'を'],\n",
       " 'Korean': ['이', '의', '다', '의 ', '에', '는', '는 ', '하', '을', '을 '],\n",
       " 'Tamil': ['்', 'க', 'ு', 'ி', 'த', '் ', 'ப', 'ம', 'ட', 'ர'],\n",
       " 'English': ['e', 'a', 't', 'i', 'o', 'n', 's', 'r', 'h', 'l'],\n",
       " 'Swedish': ['e', 'r', 'a', 'n', 't', 'i', 's', 'd', 'l', 'o'],\n",
       " 'Thai': ['า', 'น', 'ร', 'ก', 'อ', '่', 'เ', 'ง', 'ม', 'ั'],\n",
       " 'Turkish': ['a', 'e', 'i', 'n', 'r', 'l', 'ı', 'd', 'k', 't'],\n",
       " 'Portugese': ['a', 'e', 'o', 's', 'i', 'r', 'd', 'n', 't', 'm'],\n",
       " 'Arabic': ['ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا', 'ن', 'ت', 'ر'],\n",
       " 'Chinese': ['，', '的', '。', '年', '在', '、', '一', '中', 'a', 'e'],\n",
       " 'Hindi': ['ा', 'क', 'र', '्', 'े', 'ि', 'स', ' क', 'न', 'त'],\n",
       " 'Russian': ['о', 'и', 'е', 'а', 'н', 'с', 'р', 'т', 'в', 'л'],\n",
       " 'Latin': ['i', 'a', 'e', 't', 's', 'n', 'r', 'u', 'o', 'm']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=10)\n",
    "top10PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaaf4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictToArray(dict, languages=language_list):\n",
    "    '''Converts a Language dictionary to an array and removes the duplicate values'''\n",
    "    char = []\n",
    "\n",
    "    for lang in languages:\n",
    "        arr = dict[lang]\n",
    "        char = char + arr\n",
    "    \n",
    "    dict_array = list(set(char))\n",
    "\n",
    "    return dict_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1263e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['의', ' ا', '하', '、', 'د', 'த', 'l', 'स', 'ال', '는', 'க', '年', 'ه ', 'า', 'u', 'ر', 'о', 'م', 'и', '，', 'น', 'ه', 'ு', '는 ', 'ي', 'र', 'a', 'क', '一', 'ن', 'ा', 'に', '。', 'ہ', '的', 'ک', 'る', 'เ', 'ி', 'а', 'ı', '이', 'ம', 'ل', 's', 'ப', ' क', 'ا', '் ', 'ی', 'อ', 'н', '다', 'e', '中', 'ก', 'т', '่', 'i', 'े', 'с', 'ि', 'た', 'は', 'の', 't', 'n', 'と', 'r', '在', '의 ', '에', 'و', 'ร', 'е', 'o', 'р', 'ง', '்', '्', 'd']\n",
      "Total Number of Characters: 81\n"
     ]
    }
   ],
   "source": [
    "dict8_array = dictToArray(top8PerLanguage_dict)\n",
    "\n",
    "print(dict8_array)\n",
    "print(\"Total Number of Characters:\", len(dict8_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c832284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['의', ' ا', '하', '、', 'د', 'த', 'l', 'h', 'ے', 'स', 'ال', '을 ', '는', 'க', '年', 'ه ', 'า', 'u', 'ر', 'о', 'م', 'и', 'ت', '，', 'n ', 'น', 'ه', 'л', 'ு', '는 ', 'd', 'ي', 'र', 'a', 'क', '一', 'ن', 'ा', 'に', '。', 'ہ', '的', 'ک', 'る', 'เ', 'ி', 'а', 'ı', ' د', '이', 'त', 'ம', 'ー', 'न', 'ب', 'ل', 'ม', 'm', 's', 'ப', 'e ', 'を', 'c', ' क', 'k', 'ا', '் ', 'ی', 'อ', 'ั', 'н', '다', 'e', '中', 'ர', 'ก', 'т', '่', 'i', 'े', 'с', 'ि', 'た', 'は', 'の', 't', 'n', 'と', 'r', 'an', '在', 'в', '을', '의 ', '에', 'و', 'ร', 'е', 'o', 'р', 'ง', '்', '्', 'ட']\n",
      "Total Number of Characters: 104\n"
     ]
    }
   ],
   "source": [
    "dict10_array = dictToArray(top10PerLanguage_dict)\n",
    "\n",
    "print(dict10_array)\n",
    "print(\"Total Number of Characters:\", len(dict10_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58581a6a",
   "metadata": {},
   "source": [
    "## Training the Model: RandomForests\n",
    "Now that we know the top 8 and 10 used characters for each language, we can use them to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332a707",
   "metadata": {},
   "source": [
    "First, we'll use our `feature engineering` function to find the number of times our characters are used in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0d27435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictVectorizor = feature_engineering(X_train, dict8_array)\n",
    "dict8Frame = numpy_into_dataframe(dictVectorizor, dict8_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db2bf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>의</th>\n",
       "      <th>ا</th>\n",
       "      <th>하</th>\n",
       "      <th>、</th>\n",
       "      <th>د</th>\n",
       "      <th>த</th>\n",
       "      <th>l</th>\n",
       "      <th>स</th>\n",
       "      <th>ال</th>\n",
       "      <th>는</th>\n",
       "      <th>...</th>\n",
       "      <th>에</th>\n",
       "      <th>و</th>\n",
       "      <th>ร</th>\n",
       "      <th>е</th>\n",
       "      <th>o</th>\n",
       "      <th>р</th>\n",
       "      <th>ง</th>\n",
       "      <th>்</th>\n",
       "      <th>्</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039906</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17600 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         의    ا    하         、         د         த         l    स   ال    는  \\\n",
       "0      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.031496  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.000000  0.000000  0.079585  0.000000  0.0  0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "...    ...  ...  ...       ...       ...       ...       ...  ...  ...  ...   \n",
       "17595  0.0  0.0  0.0  0.000000  0.027304  0.000000  0.000000  0.0  0.0  0.0   \n",
       "17596  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.040359  0.0  0.0  0.0   \n",
       "17597  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.078283  0.0  0.0  0.0   \n",
       "17598  0.0  0.0  0.0  0.000000  0.057018  0.000000  0.000000  0.0  0.0  0.0   \n",
       "17599  0.0  0.0  0.0  0.013072  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "       ...    에         و         ร        е         o         р         ง  \\\n",
       "0      ...  0.0  0.000000  0.021622  0.00000  0.000000  0.000000  0.027027   \n",
       "1      ...  0.0  0.000000  0.039906  0.00000  0.000000  0.000000  0.035211   \n",
       "2      ...  0.0  0.000000  0.000000  0.00000  0.023622  0.000000  0.000000   \n",
       "3      ...  0.0  0.000000  0.000000  0.00000  0.003460  0.000000  0.000000   \n",
       "4      ...  0.0  0.000000  0.000000  0.07984  0.000000  0.042914  0.000000   \n",
       "...    ...  ...       ...       ...      ...       ...       ...       ...   \n",
       "17595  ...  0.0  0.051195  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "17596  ...  0.0  0.000000  0.000000  0.00000  0.076233  0.000000  0.000000   \n",
       "17597  ...  0.0  0.000000  0.000000  0.00000  0.025253  0.000000  0.000000   \n",
       "17598  ...  0.0  0.061404  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "17599  ...  0.0  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "\n",
       "              ்    ्         d  \n",
       "0      0.000000  0.0  0.000000  \n",
       "1      0.000000  0.0  0.000000  \n",
       "2      0.000000  0.0  0.039370  \n",
       "3      0.148789  0.0  0.000000  \n",
       "4      0.000000  0.0  0.000000  \n",
       "...         ...  ...       ...  \n",
       "17595  0.000000  0.0  0.000000  \n",
       "17596  0.000000  0.0  0.053812  \n",
       "17597  0.000000  0.0  0.035354  \n",
       "17598  0.000000  0.0  0.000000  \n",
       "17599  0.000000  0.0  0.000000  \n",
       "\n",
       "[17600 rows x 81 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict8Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11ef48",
   "metadata": {},
   "source": [
    "Next, a `RandomForestClassifier` model will be trained on the created dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f700401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "\n",
    "rfc.fit(dict8Frame, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778330e",
   "metadata": {},
   "source": [
    "The test set will be vectorized and predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55933584",
   "metadata": {},
   "outputs": [],
   "source": [
    "testVectorizor = feature_engineering(X_test, dict8_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f939dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grsru\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_8pred = rfc.predict(testVectorizor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c728f",
   "metadata": {},
   "source": [
    "Now, to display our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e3f7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of top 8 used characters: 0.8393181818181819\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy Score of top 8 used characters:\",accuracy_score(y_test, y_8pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b372",
   "metadata": {},
   "source": [
    "Now for the Top 10 Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0480785",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictVectorizor = feature_engineering(X_train, dict10_array)\n",
    "dict10Frame = numpy_into_dataframe(dictVectorizor, dict10_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec729cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>의</th>\n",
       "      <th>ا</th>\n",
       "      <th>하</th>\n",
       "      <th>、</th>\n",
       "      <th>د</th>\n",
       "      <th>த</th>\n",
       "      <th>l</th>\n",
       "      <th>h</th>\n",
       "      <th>ے</th>\n",
       "      <th>स</th>\n",
       "      <th>...</th>\n",
       "      <th>에</th>\n",
       "      <th>و</th>\n",
       "      <th>ร</th>\n",
       "      <th>е</th>\n",
       "      <th>o</th>\n",
       "      <th>р</th>\n",
       "      <th>ง</th>\n",
       "      <th>்</th>\n",
       "      <th>्</th>\n",
       "      <th>ட</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039906</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.011811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.07984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17600 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         의    ا    하         、         د         த         l         h    ے  \\\n",
       "0      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "1      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "2      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.031496  0.011811  0.0   \n",
       "3      0.0  0.0  0.0  0.000000  0.000000  0.079585  0.000000  0.003460  0.0   \n",
       "4      0.0  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "...    ...  ...  ...       ...       ...       ...       ...       ...  ...   \n",
       "17595  0.0  0.0  0.0  0.000000  0.027304  0.000000  0.000000  0.000000  0.0   \n",
       "17596  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.040359  0.004484  0.0   \n",
       "17597  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.078283  0.005051  0.0   \n",
       "17598  0.0  0.0  0.0  0.000000  0.057018  0.000000  0.000000  0.000000  0.0   \n",
       "17599  0.0  0.0  0.0  0.013072  0.000000  0.000000  0.000000  0.000000  0.0   \n",
       "\n",
       "         स  ...    에         و         ร        е         o         р  \\\n",
       "0      0.0  ...  0.0  0.000000  0.021622  0.00000  0.000000  0.000000   \n",
       "1      0.0  ...  0.0  0.000000  0.039906  0.00000  0.000000  0.000000   \n",
       "2      0.0  ...  0.0  0.000000  0.000000  0.00000  0.023622  0.000000   \n",
       "3      0.0  ...  0.0  0.000000  0.000000  0.00000  0.003460  0.000000   \n",
       "4      0.0  ...  0.0  0.000000  0.000000  0.07984  0.000000  0.042914   \n",
       "...    ...  ...  ...       ...       ...      ...       ...       ...   \n",
       "17595  0.0  ...  0.0  0.051195  0.000000  0.00000  0.000000  0.000000   \n",
       "17596  0.0  ...  0.0  0.000000  0.000000  0.00000  0.076233  0.000000   \n",
       "17597  0.0  ...  0.0  0.000000  0.000000  0.00000  0.025253  0.000000   \n",
       "17598  0.0  ...  0.0  0.061404  0.000000  0.00000  0.000000  0.000000   \n",
       "17599  0.0  ...  0.0  0.000000  0.000000  0.00000  0.000000  0.000000   \n",
       "\n",
       "              ง         ்    ्         ட  \n",
       "0      0.027027  0.000000  0.0  0.000000  \n",
       "1      0.035211  0.000000  0.0  0.000000  \n",
       "2      0.000000  0.000000  0.0  0.000000  \n",
       "3      0.000000  0.148789  0.0  0.069204  \n",
       "4      0.000000  0.000000  0.0  0.000000  \n",
       "...         ...       ...  ...       ...  \n",
       "17595  0.000000  0.000000  0.0  0.000000  \n",
       "17596  0.000000  0.000000  0.0  0.000000  \n",
       "17597  0.000000  0.000000  0.0  0.000000  \n",
       "17598  0.000000  0.000000  0.0  0.000000  \n",
       "17599  0.000000  0.000000  0.0  0.000000  \n",
       "\n",
       "[17600 rows x 104 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict10Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b8084b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "\n",
    "rfc2.fit(dict10Frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b9aaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2Vectorizor = feature_engineering(X_test, dict10_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b41d0750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grsru\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_10pred = rfc2.predict(test2Vectorizor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "463588d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of top 10 used characters: 0.8872727272727273\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score of top 10 used characters:\",accuracy_score(y_test, y_10pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b05f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these arrays into dictonaries\n",
    "english_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "estonian_alpha = [A, B, D, E, F, G, H, I, J, K, L, M, N, O, P, R, S, Š, Z, Ž, T, U, V, Õ, Ä, Ö, Ü]\n",
    "swedish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, å, ä, ö]\n",
    "thai_alpha = [ก, ข, ค, ฅ, ฆ, ง, จ, ฉ, ช, ฌ, ญ, ฎ, ฏ, ฐ, ฑ, ฒ, ณ, ด, ต, ถ, ท, ธ, น, บ, บ, ผ, ฝ, พ, ฟ, ภ, \n",
    "               ม, ย, ร, ล, ว, ศ, ษ, ส, ห, ฬ, อ, ฮ] \n",
    "tamil_alpha = [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ங, ச, ஞ, ட, ண, த, ந, ன, ப, ம, ய, ர, ற, ல, ள, ழ, வ]\n",
    "dutch_alpha = english_alpha\n",
    "japanese_alpha = [ぁ, あ, ぃ, い, ぅ, う, ぇ, え, ぉ, お, か, が, き, ぎ, く, ぐ, け, げ, こ, ご, さ, ざ, し, じ, す, ず,\n",
    "                  せ, ぜ, そ, ぞ, た, だ, ち, ぢ, っ, つ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ,\n",
    "                  ふ, ぶ, ぷ, へ, べ, れ, る, り, ら, よ, ょ, ゆ, ゅ, や, ゃ, も, め, む, み, ま, ぽ, ぼ, ほ, ぺ, ろ, ゎ,\n",
    "                  わ, ゐ, ゑ, を, ん, ゔ, ゕ, ゖ,  ゚, ゛, ゜, ゝ, ゞ, ゟ, ゠, ァ, ア, サ, ゴ, コ, ゲ, ケ, グ, ク, ギ, キ,\n",
    "                  ガ, カ, オ, ォ, エ, ェ, ウ, ゥ, イ, ィ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ ,ダ ,チ ,ヂ, ッ, ツ, ヅ,\n",
    "                  テ, デ, ト, ホ, ペ, ベ, ヘ, プ, ブ, フ, ピ, ビ, ヒ, パ, バ, ハ, ノ, ネ, ヌ, ニ, ナ, ド, ボ, ポ, マ, ミ, \n",
    "                  ム, メ, モ, ャ, ヤ, ュ, ユ, ョ, ヨ, ラ, リ, ル, レ, ロ, ヮ, ㍿, ㍐, ヿ, ヾ, ヽ, ー, ・, ヺ, ヹ, ヸ, ヷ,\n",
    "                  ヶ, ヵ, ヴ, ン, ヲ, ヱ, ヰ, ワ]\n",
    "turkish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, r, s, t, u, v, y, z, ç, ğ, ı, İ, î, ö, ş, ü]\n",
    "latin_alpha = english_alpha\n",
    "urdu_alpha = [ش,س,ژ,ز,ڑ,ر,ذ,ڈ,د,خ,ح,چ,\n",
    "              ج,ث,ٹ,ت,پ,ب,آ,ا,ے,ی,ھ,ہ,و,ں,ن,م,ل,گ,ک,ق,ف,غ,ع,ظ,ط,ض,ص]\n",
    "indonesian_alpha = english_alpha\n",
    "portuguese_alpha = [ç, á, é, í, ó, ú, â, ê, ô, ã, õ, à, è, ì, ò, ù, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "french_alpha = [ç, é, â, ê, î, ô, û, à, è, ì, ò, ù, ë, ï, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "chinese_alpha = [胡, 赛, 尼, 本, 人, 和, 小, 说, 的, 主, 人, 公, 阿, 米, 尔, 一, 样, 都, 是, 出, 生, 在, 阿, 富, 汗, 首, 都, \n",
    "                 喀, 布, 尔, 少, 年, 时, 代, 便, 离, 开, 了, 这, 个, 国, 家, 。, 胡, 赛, 尼, 直, 到, 年, 小, 说, 出, 版, 之, \n",
    "                 后, 才, 首, 次, 回, 到, 已, 经, 离, 开, 年, 的, 祖, 国, 。, 他, 在, 苏, 联, 入, 侵, 时, 离, 开, 了, 阿, 富, \n",
    "                 汗, 而, 他, 的, 很, 多, 童, 年, 好, 友, 在, 阿, 富, 汗, 生, 活, 在, 他, 们, 出, 发, 之, 前, 罗, 伯, 特, 伊,\n",
    "                 达, 尔, 文, 卷, 查, 尔, 斯, 赖, 尔, 所, 著, 地, 质, 学, 原, 理, 在, 南, 美, 他, 得, 到, 第, 卷, 该, 书, 将, \n",
    "                 地, 形, 地, 貌, 解, 释, 为, 漫, 长, 历, 史, 时, 间, 渐, 进, 演, 变, 的, 的, 结, 果, 当, 他, 旅, 程, 的, 第, \n",
    "                 站, 抵, 达, 圣, 地, 亚, 哥, 佛, 得, 角, 的, 时, 候, 达, 尔, 文]\n",
    "korean_alpha = [ᄁ,ᄂ,ᄃ,ᄄ,ᄅᄆᄇ,ᄈ,ᄉ,ᄊ,ᄋ,ᄌᄍ,ᄎ,ᄏ,ᄐ,ᄑᄒ,아,악,안,알,암,압,앙,앞애,액,앵야,얀,약,양,얘,어,억,\n",
    "                언,얼,엄,업,엉,에,여,역,연,열,염,엽,영,예,ᄀ,여,역,연,열,염,엽,영,예,오,옥,온,올,옴,옹,와,완,왈,왕,왜,외,왼,\n",
    "                요,욕,용,우,욱,운,울,움,웅,워,원,월,위,유,육,윤,율,융,윷,으,은,을,음읍,응,의,이,익,인,일,임,입,잉,잎]\n",
    "hindi_alpha = [ऄ, अ, आ, इ, ई, उ, ऊ, ऋ, ऌ, ऍ, ऎ, ए, ऐ, ऑ, ऒ, ओ, औ, क, ख, ग, घ, ङ, च, छ, ज, झ, प, ऩ, न, ध, द, \n",
    "               थ, त, ण, ढ, ड, ठ, ट, ञ, फ, ब, भ, म, य, र, ऱ, ल, ळ, ऴ, व, श, ष, ४, ३, २, १, ०, ॥, ।, ॡ, ॠ, ॐ, ऽ, \n",
    "               ह, स, ५, ६, ७, ॲ, ॳ, ॴ, ॵ, ॶ, ॷ, ॹ, ॺ, ॻ, ॼ, ॾ, ॿ, ೱऀँं, ः, ऺ, ऻ, ा, ि, ी, ॎ, ॏॕैेॣॢ, ॗ]\n",
    "spanish_alpha = [á, é, í, ó, ú, ñ, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "pushto_alpha = [,ﺏ ,پ ,ﺕ ,ټ ,ﺙ ,ﺝ ,چ ,ﺡ ,ﺥ ,څ ,ځ ,ﺩ ,ډ ,ﺫ ,ﺭ ,ړ ,ﺯ ,ژ ,ږ ,ﺱ ,ﺵ ,ښ ,ﺹ ,ﺽ ,ﻁ ,ﻅ ,ﻉ ,ﻍ ,ﻑ ,ﻕ ,ک ,ګ ,ﻝ ,ﻡ ,ﻥ ,ڼ, ,ﻭ ,ه ,ۀ ,ي ,ې ,ی ,ۍ ,ئ]\n",
    "persian_alpha = [,ش,س,ژ,ز,ر,ذ,د,خ,ح,چ,ج,ث,ت,پ,ب,آ,ا,ص,ض,ط,ظ,ع,غ,ف,ق,ک,گ,ل,م,ن,و,ه,ی]\n",
    "romanian_alpha = [ă, â, î, ș, ş, ț, ţ, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "russian_alpha = [б, в, г, д, ж, з, к, л, м, н, п, р, с, т, ф, х, ц, ч, ш, щ, а, е, ё, и, о, у, ы, э, ю, я, й]\n",
    "arabic_alpha = [ش,س,ز,ر,ذ,د,خ,ح,ج,ث,ت,ب,ا,ء,ي,و,ه,ن,م,ل,ك,ق,ف,غ,ع,ظ,ط,ض,ص]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Average Word Length of a single string\n",
    "def avg_word_len(string):\n",
    "    # Split string up and find total amount of words present\n",
    "    words = string.split()\n",
    "    wordCount = len(words)\n",
    "    \n",
    "    # Calculate actual average  \n",
    "    ch = 0\n",
    "    for word in words:\n",
    "        ch += len(word) # Add up all chars\n",
    "    avg = ch / wordCount # Divide sum of chars by amount of words present\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6809f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Average Sentence Length across a piece of text\n",
    "def avg_sent_len(text):\n",
    "  sentences = text.split(\".\") # Split the text into a list of sentences.\n",
    "  words = text.split(\" \") # split the input text into a list of separate words\n",
    "  if(sentences[len(sentences)-1]==\"\"): # if the last value in sentences is an empty string\n",
    "    average_sentence_length = len(words) / len(sentences)-1\n",
    "  else:\n",
    "    average_sentence_length = len(words) / len(sentences)\n",
    "  return average_sentence_length # returning avg length of sentence\n",
    "  \n",
    "ans = avg_sent_len(\"I am going.to see you later\") # function call\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test this function (avg_sent_len)          #Mr., Dr., Ms., etc. are words that may be a problem for this function\n",
    "avg_sent_len(input(\"Provide a body of text: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b7e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encoding                                #Attempt failed lol\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder() # instantiate one hot encoder\n",
    "cat_encoder2 = cat_encoder.fit_transform(data_trans)\n",
    "cat_encoder2 #sparse matrix\n",
    "\n",
    "cat_encoder2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "features = pd.get_dummies(data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c881dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_ أ</th>\n",
       "      <th>1_ ہ</th>\n",
       "      <th>2_या</th>\n",
       "      <th>3_ د</th>\n",
       "      <th>4_行</th>\n",
       "      <th>5_》</th>\n",
       "      <th>6_こ</th>\n",
       "      <th>7_ng</th>\n",
       "      <th>8_े</th>\n",
       "      <th>9_अ</th>\n",
       "      <th>...</th>\n",
       "      <th>554_ட</th>\n",
       "      <th>555_்க</th>\n",
       "      <th>556_ा</th>\n",
       "      <th>557_서</th>\n",
       "      <th>558_을</th>\n",
       "      <th>559_த்</th>\n",
       "      <th>560_ु</th>\n",
       "      <th>561_t</th>\n",
       "      <th>562_も</th>\n",
       "      <th>563_ं</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chars</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0_ أ  1_ ہ  2_या  3_ د  4_行  5_》  6_こ  7_ng  8_े  9_अ  ...  554_ட  \\\n",
       "Chars     1     1     1     1    1    1    1     1    1    1  ...      1   \n",
       "\n",
       "       555_்க  556_ा   557_서   558_을   559_த்  560_ु  561_t  562_も  563_ं   \n",
       "Chars       1       1       1       1       1      1      1      1       1  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chars</th>\n",
       "      <td>ed</td>\n",
       "      <td>た。</td>\n",
       "      <td>m</td>\n",
       "      <td>ی</td>\n",
       "      <td>nd</td>\n",
       "      <td>d</td>\n",
       "      <td>と</td>\n",
       "      <td>د</td>\n",
       "      <td>ار</td>\n",
       "      <td>和</td>\n",
       "      <td>...</td>\n",
       "      <td>ใ</td>\n",
       "      <td>ไ</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>่</td>\n",
       "      <td>क</td>\n",
       "      <td>อง</td>\n",
       "      <td>‌</td>\n",
       "      <td>有</td>\n",
       "      <td>و</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9    ... 554 555 556 557 558 559  \\\n",
       "Chars  ed  た。   m  ی   nd   d   と  د   ار   和  ...   ใ   ไ   h   d   ่   क   \n",
       "\n",
       "      560 561 562 563  \n",
       "Chars  อง   ‌   有   و  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9c02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2d7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
