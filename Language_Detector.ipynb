{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd55be",
   "metadata": {},
   "source": [
    "# Understanding the Kaggle Data\n",
    "The dataset acquired from [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/data) contains words from several different languages. The noise contained in the dataset are duplicate words. To reduce this noise, the words will be broken down into single and double characters, then rated based on how often they show up in that respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data*\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18753df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "language_list = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "017c263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "                               ...                        \n",
       "11964    باباجان غفورف تاریخ‌دان و نویسندهٔ کتاب تاریخ ...\n",
       "21575    en  fue invitado por fernando ii para ocupar l...\n",
       "5390     doğu kanada atabasklarına geleneksel olarak dü...\n",
       "860      پژواک د يوې ځانگړې پروژې په توگه د اساسي قانون...\n",
       "15795    テンサイについては糖分を高度に精製する必要があることからサトウキビと同じような黒糖を作るのは...\n",
       "Name: Text, Length: 17600, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b99e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "            \n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "679218c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_2(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['english', 'estonian', 'swedish', 'thai', 'tamil', 'dutch', 'japanese', 'turkish', 'latin', 'urdu',\n",
    "             'indonesian', 'portuguese', 'french', 'chinese', 'korean', 'hindi', 'spanish', 'pushto', 'persian',\n",
    "             'romanian', 'russian', 'arabic']\n",
    "    \n",
    "    data_frame = pd.DataFrame(new_arr, columns = names)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e624396a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>estonian</th>\n",
       "      <th>swedish</th>\n",
       "      <th>thai</th>\n",
       "      <th>tamil</th>\n",
       "      <th>dutch</th>\n",
       "      <th>japanese</th>\n",
       "      <th>turkish</th>\n",
       "      <th>latin</th>\n",
       "      <th>urdu</th>\n",
       "      <th>...</th>\n",
       "      <th>french</th>\n",
       "      <th>chinese</th>\n",
       "      <th>korean</th>\n",
       "      <th>hindi</th>\n",
       "      <th>spanish</th>\n",
       "      <th>pushto</th>\n",
       "      <th>persian</th>\n",
       "      <th>romanian</th>\n",
       "      <th>russian</th>\n",
       "      <th>arabic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192913</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english  estonian   swedish      thai     tamil     dutch  japanese  \\\n",
       "0  0.000000  0.000000  0.000000  0.118919  0.000000  0.000000       0.0   \n",
       "1  0.000000  0.000000  0.000000  0.157277  0.000000  0.000000       0.0   \n",
       "2  0.523622  0.263780  0.523622  0.000000  0.000000  0.523622       0.0   \n",
       "3  0.038062  0.013841  0.031142  0.000000  0.034602  0.038062       0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000       0.0   \n",
       "\n",
       "    turkish     latin  urdu  ...  french  chinese  korean  hindi   spanish  \\\n",
       "0  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "1  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "2  0.192913  0.598425   0.0  ...     0.0      0.0     0.0    0.0  0.208661   \n",
       "3  0.006920  0.031142   0.0  ...     0.0      0.0     0.0    0.0  0.013841   \n",
       "4  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "\n",
       "   pushto  persian  romanian   russian  arabic  \n",
       "0     0.0      0.0       0.0  0.000000     0.0  \n",
       "1     0.0      0.0       0.0  0.000000     0.0  \n",
       "2     0.0      0.0       0.0  0.000000     0.0  \n",
       "3     0.0      0.0       0.0  0.000000     0.0  \n",
       "4     0.0      0.0       0.0  0.264471     0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]\n",
    "\n",
    "panda = X_train.head()\n",
    "some_data = feature_engineering_2(panda, new_chars)\n",
    "some_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec7ed57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8252272727272727\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = X_train.head(6000)\n",
    "X_train_2_eng = feature_engineering_2(X_train_2, new_chars)\n",
    "\n",
    "y_train_2 = y_train.head(6000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf_2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf_2.fit(X_train_2_eng, y_train_2)\n",
    "\n",
    "X_test_2 = feature_engineering_2(X_test, new_chars)\n",
    "y_preds_2 = rnd_clf_2.predict(X_test_2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score_2 = accuracy_score(y_test, y_preds_2)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa73b2",
   "metadata": {},
   "source": [
    "### First Test Model\n",
    "----\n",
    "This is a test model to experiment how to implement the finalized model onto the flask website (via pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cb9187aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8822727272727273\n"
     ]
    }
   ],
   "source": [
    "#characters for first model\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "X_train_2 = X_train.head(3000)\n",
    "model_one = feature_engineering(X_train_2, chars_2)\n",
    "model_one\n",
    "\n",
    "y_train_one = y_train.head(3000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(model_one, y_train_one)\n",
    "\n",
    "X_test_1 = feature_engineering(X_test, chars_2)\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))\n",
    "\n",
    "import pickle #ask about pickle\n",
    "\n",
    "saved_model = pickle.dumps(rnd_clf)\n",
    "\n",
    "rdf_from_pickle = pickle.loads(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed5739d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = X_train.head()\n",
    "panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce97ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_22_features = ['e', 't', 'a', 'i', 'o', 'á', 'é', 'í']\n",
    "\n",
    "test_data = feature_engineering(panda, first_22_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cfa4cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>t</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>o</th>\n",
       "      <th>á</th>\n",
       "      <th>é</th>\n",
       "      <th>í</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.023622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          e         t         a         i         o    á    é    í\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0\n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0\n",
       "2  0.106299  0.051181  0.086614  0.051181  0.023622  0.0  0.0  0.0\n",
       "3  0.000000  0.006920  0.006920  0.006920  0.003460  0.0  0.0  0.0\n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.0  0.0  0.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2a28",
   "metadata": {},
   "source": [
    "## Creating Features: Most Used Characters\n",
    "To train the models to determine which language is being used by the user, we first need to know which characters are used in each language. The best approach for this is to used the dataset, _which is already using the characters from each language_, and find the **most used characters** in them.\n",
    "\n",
    "### Kaggle Resources\n",
    "----\n",
    "The following code is taken from the [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/notebook) to understand how to process characters for each language. Kaggle uses the `CountVectorizor` Method from the `sklearn` module to tokenize the characters into readable 1's and 0's. Then, it counts how many times that character has been used in the sample data provided.\n",
    "\n",
    "This method reduces the necessity of locating alphabets for each language and creating custom functions to find the most used characters in the dataset.\n",
    " \n",
    "**To summarize Kaggle's findings: Languages based off the Latin Alphabet are easier to differentiate from each other in the data set while Languages with their own Alphabet, _like Chinese and Japanese_, can be differentiated by single characters alone.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3797753",
   "metadata": {},
   "source": [
    "This code here fits and transforms the X_train and X_test data sets into readable 1's and 0's and counts the number of times a specific character shows up in the datasets.The `min_df` parameter tells `CountVectorizor` to save any characters that are used **at least 1% of the time** in the dataset.\n",
    "\n",
    "These matrices are saved as `X_top1Percent_train_raw` and `X_top1Percent_test_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb378224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a list of single and double characters from the top 1% of to be used as features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b0c14",
   "metadata": {},
   "source": [
    "The `train_lang_dict()` function takes in the raw vectorized X_train and y_train data sets and converts them into readable dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a14148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Command from Kaggle connects the character features to their specific language\n",
    "\n",
    "# Aggregate Unigrams per language\n",
    "def train_lang_dict(X_raw_counts, y_train):\n",
    "    lang_dict = {}\n",
    "    for i in range(len(y_train)):\n",
    "        lang = y_train[i]\n",
    "        v = np.array(X_raw_counts[i])\n",
    "        if not lang in lang_dict:\n",
    "            lang_dict[lang] = v\n",
    "        else:\n",
    "            lang_dict[lang] += v\n",
    "            \n",
    "    # to relative\n",
    "    for lang in lang_dict:\n",
    "        v = lang_dict[lang]\n",
    "        lang_dict[lang] = v / np.sum(v)\n",
    "        \n",
    "    return lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c26b5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2, max_df=.9)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b62b41a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>（</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3075</th>\n",
       "      <td>）。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>，</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>：</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3078 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      \"\n",
       "1      -\n",
       "2      [\n",
       "3      a\n",
       "4      b\n",
       "...   ..\n",
       "3073   （\n",
       "3074   ）\n",
       "3075  ）。\n",
       "3076   ，\n",
       "3077   ：\n",
       "\n",
       "[3078 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n",
    "\n",
    "top1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names_out()\n",
    "\n",
    "pd.DataFrame(top1PercentFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d5e458c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thai': array([4.17770312e-04, 2.00271335e-04, 1.29207313e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Swedish': array([1.08294023e-04, 3.09720905e-04, 6.49764136e-06, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Tamil': array([8.97649203e-05, 2.33806304e-04, 1.67004503e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Russian': array([2.04983331e-05, 2.81386209e-04, 1.67713635e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Urdu': array([1.27741205e-04, 5.50608643e-05, 1.54170420e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Chinese': array([0.00013874, 0.00019077, 0.        , ..., 0.00079775, 0.06036852,\n",
       "        0.00202905]),\n",
       " 'Spanish': array([2.13374681e-04, 1.49362276e-04, 8.89061169e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'English': array([5.62921665e-04, 1.82022574e-04, 2.35955189e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Persian': array([1.39777544e-05, 1.08327597e-04, 1.04833158e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Pushto': array([6.86916121e-05, 5.13625918e-04, 7.80586501e-06, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Romanian': array([1.72941167e-04, 3.50049590e-04, 1.25017711e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Arabic': array([5.09491904e-04, 1.95421552e-04, 2.79173646e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Portugese': array([3.29991795e-04, 1.92643859e-04, 2.49723520e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Turkish': array([3.92358406e-04, 2.41309630e-04, 9.21029121e-06, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Estonian': array([4.32378665e-04, 1.20976414e-04, 2.46433436e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Hindi': array([2.04801733e-04, 2.22157812e-04, 2.42985107e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Dutch': array([1.53033246e-04, 9.58583252e-04, 8.28930085e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Latin': array([0.00024306, 0.0003125 , 0.00010634, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " 'French': array([2.46755604e-05, 3.10207046e-04, 1.76254003e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Korean': array([9.32727675e-04, 3.10909225e-04, 4.74268309e-05, ...,\n",
       "        0.00000000e+00, 2.63482394e-04, 3.68875352e-05]),\n",
       " 'Indonesian': array([2.60997864e-04, 1.54468124e-04, 2.48569394e-05, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]),\n",
       " 'Japanese': array([7.98427712e-05, 9.15120993e-04, 0.00000000e+00, ...,\n",
       "        6.75592679e-04, 2.45670065e-05, 2.76378823e-04])}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_dict_top1Percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f036f3",
   "metadata": {},
   "source": [
    "The `getRelevantGramsPerLanguage()` function processes the dictionary and returns a dictionary with only the top 50 **most used** characters for **each** language. This number _can_ be changed by setting `top=x` when you call `getRelevantGramsPerLanguage()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0e18e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantGramsPerLanguage(features, language_dict, languages, top=50):\n",
    "    relevantGramsPerLanguage = {}\n",
    "    for lang in languages:\n",
    "        chars = []\n",
    "        relevantGramsPerLanguage[lang] = chars\n",
    "        v = language_dict[lang]\n",
    "        sortIndex = (-v).argsort()[:top]\n",
    "        for i in range(len(sortIndex)):\n",
    "            chars.append(features[sortIndex[i]])\n",
    "    return relevantGramsPerLanguage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae2d19",
   "metadata": {},
   "source": [
    "Below Displays the top 8 and 10 Characters from each language in a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a571530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Swedish': ['e', 'r', 'a', 'n', 't', 'i', 's', 'd'],\n",
       " 'Arabic': ['ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا', 'ن'],\n",
       " 'Turkish': ['a', 'e', 'i', 'n', 'r', 'l', 'ı', 'd'],\n",
       " 'Persian': ['ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و', 'م'],\n",
       " 'Tamil': ['்', 'க', 'ு', 'ி', 'த', '் ', 'ப', 'ம'],\n",
       " 'Urdu': ['ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن', 'ہ'],\n",
       " 'Romanian': ['e', 'a', 'i', 'r', 'n', 't', 'u', 'l'],\n",
       " 'Spanish': ['e', 'a', 'o', 'n', 's', 'r', 'i', 'l'],\n",
       " 'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と'],\n",
       " 'Chinese': ['，', '的', '。', '年', '在', '、', '一', '中'],\n",
       " 'Thai': ['า', 'น', 'ร', 'ก', 'อ', '่', 'เ', 'ง'],\n",
       " 'Portugese': ['a', 'e', 'o', 's', 'i', 'r', 'd', 'n'],\n",
       " 'French': ['e', 'a', 'n', 's', 'i', 'r', 't', 'l'],\n",
       " 'Indonesian': ['a', 'n', 'e', 'i', 'r', 'u', 't', 's'],\n",
       " 'Korean': ['이', '의', '다', '의 ', '에', '는', '는 ', '하'],\n",
       " 'Estonian': ['a', 'i', 'e', 's', 't', 'l', 'n', 'u'],\n",
       " 'Pushto': ['و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر', 'ل'],\n",
       " 'Russian': ['о', 'и', 'е', 'а', 'н', 'с', 'р', 'т'],\n",
       " 'English': ['e', 'a', 't', 'i', 'o', 'n', 's', 'r'],\n",
       " 'Dutch': ['e', 'n', 'a', 'i', 'r', 't', 'o', 'd'],\n",
       " 'Hindi': ['ा', 'क', 'र', '्', 'े', 'ि', 'स', ' क'],\n",
       " 'Latin': ['i', 'a', 'e', 't', 's', 'n', 'r', 'u']}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top8PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=8)\n",
    "top8PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01d01a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Swedish': ['e', 'r', 'a', 'n', 't', 'i', 's', 'd', 'l', 'o'],\n",
       " 'Arabic': ['ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا', 'ن', 'ت', 'ر'],\n",
       " 'Turkish': ['a', 'e', 'i', 'n', 'r', 'l', 'ı', 'd', 'k', 't'],\n",
       " 'Persian': ['ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و', 'م', 'ت', 'ب'],\n",
       " 'Tamil': ['்', 'க', 'ு', 'ி', 'த', '் ', 'ப', 'ம', 'ட', 'ர'],\n",
       " 'Urdu': ['ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن', 'ہ', 'ے', 'ل'],\n",
       " 'Romanian': ['e', 'a', 'i', 'r', 'n', 't', 'u', 'l', 'o', 'c'],\n",
       " 'Spanish': ['e', 'a', 'o', 'n', 's', 'r', 'i', 'l', 'd', 't'],\n",
       " 'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と', 'ー', 'を'],\n",
       " 'Chinese': ['，', '的', '。', '年', '在', '、', '一', '中', 'a', 'e'],\n",
       " 'Thai': ['า', 'น', 'ร', 'ก', 'อ', '่', 'เ', 'ง', 'ม', 'ั'],\n",
       " 'Portugese': ['a', 'e', 'o', 's', 'i', 'r', 'd', 'n', 't', 'm'],\n",
       " 'French': ['e', 'a', 'n', 's', 'i', 'r', 't', 'l', 'e ', 'u'],\n",
       " 'Indonesian': ['a', 'n', 'e', 'i', 'r', 'u', 't', 's', 'an', 'k'],\n",
       " 'Korean': ['이', '의', '다', '의 ', '에', '는', '는 ', '하', '을', '을 '],\n",
       " 'Estonian': ['a', 'i', 'e', 's', 't', 'l', 'n', 'u', 'o', 'k'],\n",
       " 'Pushto': ['و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر', 'ل', 'ن', ' د'],\n",
       " 'Russian': ['о', 'и', 'е', 'а', 'н', 'с', 'р', 'т', 'в', 'л'],\n",
       " 'English': ['e', 'a', 't', 'i', 'o', 'n', 's', 'r', 'h', 'l'],\n",
       " 'Dutch': ['e', 'n', 'a', 'i', 'r', 't', 'o', 'd', 's', 'n '],\n",
       " 'Hindi': ['ा', 'क', 'र', '्', 'े', 'ि', 'स', ' क', 'न', 'त'],\n",
       " 'Latin': ['i', 'a', 'e', 't', 's', 'n', 'r', 'u', 'o', 'm']}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=10)\n",
    "top10PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eaaf4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictToArray(dict, languages=language_list):\n",
    "    '''Converts a Language dictionary to an array and removes the duplicate values'''\n",
    "    char = []\n",
    "\n",
    "    for lang in languages:\n",
    "        arr = dict[lang]\n",
    "        char = char + arr\n",
    "    \n",
    "    dict_array = list(set(char))\n",
    "\n",
    "    return dict_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1263e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['م', 'ு', 'l', 'ر', 'و', 'ப', 'た', 'と', ' ا', 'ा', '다', '் ', 'с', 'ي', 'ி', '는', 'เ', '年', '에', 'ه ', 'о', '中', '在', '하', '，', 'ि', 'る', 'ı', 's', 'ہ', '的', '्', 'и', '이', 'อ', 'a', 'ل', 'े', 'т', 'க', 'น', '의 ', '의', 'ال', '는 ', 'ا', 'е', 'u', ' क', 'o', 'ن', '、', 'а', 'स', 'の', 'ம', 'ه', 'ง', 't', 'ก', 'd', 'р', '。', 'ی', 'า', 'क', 'は', 'n', '一', 'ک', 'に', 'i', 'र', 'н', 'e', '่', 'د', 'ร', 'த', 'r', '்']\n",
      "Total Number of Characters: 81\n"
     ]
    }
   ],
   "source": [
    "dict8_array = dictToArray(top8PerLanguage_dict)\n",
    "\n",
    "print(dict8_array)\n",
    "print(\"Total Number of Characters:\", len(dict8_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c832284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['م', 'ு', 'l', 'ر', 'त', 'و', 'ப', 'た', 'ม', 'в', 'と', ' ا', 'ा', 'ั', 'e ', '다', '் ', 'с', 'ர', 'k', 'ي', 'ி', 'h', '는', 'เ', ' د', '年', '에', 'ه ', 'о', '中', '在', 'л', '하', '，', 'ि', 'る', 'ı', 's', 'ہ', 'an', 'ட', '的', '्', 'и', '이', 'ت', 'อ', 'a', 'ل', 'े', 'т', 'க', 'न', 'น', '의 ', '의', 'ال', '는 ', 'を', 'ا', 'е', 'u', ' क', 'o', 'ن', '、', 'а', 'स', 'の', 'ب', 'ம', 'ه', 'ง', 'c', 't', 'ก', 'd', 'р', '。', 'ی', 'า', 'क', '을', 'は', 'n', '一', 'ک', 'に', 'i', '을 ', 'n ', 'र', 'н', 'ے', 'e', '่', 'د', 'ー', 'ร', 'த', 'r', 'm', '்']\n",
      "Total Number of Characters: 104\n"
     ]
    }
   ],
   "source": [
    "dict10_array = dictToArray(top10PerLanguage_dict)\n",
    "\n",
    "print(dict10_array)\n",
    "print(\"Total Number of Characters:\", len(dict10_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58581a6a",
   "metadata": {},
   "source": [
    "## Training the Model: RandomForests\n",
    "Now that we know the top 8 and 10 used characters for each language, we can use them to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332a707",
   "metadata": {},
   "source": [
    "First, we'll use our `feature engineering` function to find the number of times our characters are used in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f0d27435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict8Frame = feature_engineering(X_train, dict8_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2db2bf91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>م</th>\n",
       "      <th>ு</th>\n",
       "      <th>l</th>\n",
       "      <th>ر</th>\n",
       "      <th>و</th>\n",
       "      <th>ப</th>\n",
       "      <th>た</th>\n",
       "      <th>と</th>\n",
       "      <th>ا</th>\n",
       "      <th>ा</th>\n",
       "      <th>...</th>\n",
       "      <th>i</th>\n",
       "      <th>र</th>\n",
       "      <th>н</th>\n",
       "      <th>e</th>\n",
       "      <th>่</th>\n",
       "      <th>د</th>\n",
       "      <th>ร</th>\n",
       "      <th>த</th>\n",
       "      <th>r</th>\n",
       "      <th>்</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090551</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.148789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>0.040956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.051195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17600 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              م         ு         l         ر         و         ப         た  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2      0.000000  0.000000  0.031496  0.000000  0.000000  0.000000  0.000000   \n",
       "3      0.000000  0.044983  0.000000  0.000000  0.000000  0.048443  0.000000   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "17595  0.040956  0.000000  0.000000  0.061433  0.051195  0.000000  0.000000   \n",
       "17596  0.000000  0.000000  0.040359  0.000000  0.000000  0.000000  0.000000   \n",
       "17597  0.000000  0.000000  0.078283  0.000000  0.000000  0.000000  0.000000   \n",
       "17598  0.017544  0.000000  0.000000  0.043860  0.061404  0.000000  0.000000   \n",
       "17599  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.006536   \n",
       "\n",
       "             と    ا    ा  ...         i    र         н         e         ่  \\\n",
       "0      0.00000  0.0  0.0  ...  0.000000  0.0  0.000000  0.000000  0.032432   \n",
       "1      0.00000  0.0  0.0  ...  0.000000  0.0  0.000000  0.000000  0.042254   \n",
       "2      0.00000  0.0  0.0  ...  0.051181  0.0  0.000000  0.106299  0.000000   \n",
       "3      0.00000  0.0  0.0  ...  0.006920  0.0  0.000000  0.000000  0.000000   \n",
       "4      0.00000  0.0  0.0  ...  0.000000  0.0  0.063872  0.000000  0.000000   \n",
       "...        ...  ...  ...  ...       ...  ...       ...       ...       ...   \n",
       "17595  0.00000  0.0  0.0  ...  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "17596  0.00000  0.0  0.0  ...  0.067265  0.0  0.000000  0.085202  0.000000   \n",
       "17597  0.00000  0.0  0.0  ...  0.060606  0.0  0.000000  0.101010  0.000000   \n",
       "17598  0.00000  0.0  0.0  ...  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "17599  0.03268  0.0  0.0  ...  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "              د         ร         த         r         ்  \n",
       "0      0.000000  0.021622  0.000000  0.000000  0.000000  \n",
       "1      0.000000  0.039906  0.000000  0.000000  0.000000  \n",
       "2      0.000000  0.000000  0.000000  0.090551  0.000000  \n",
       "3      0.000000  0.000000  0.079585  0.003460  0.148789  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "17595  0.027304  0.000000  0.000000  0.000000  0.000000  \n",
       "17596  0.000000  0.000000  0.000000  0.076233  0.000000  \n",
       "17597  0.000000  0.000000  0.000000  0.098485  0.000000  \n",
       "17598  0.057018  0.000000  0.000000  0.000000  0.000000  \n",
       "17599  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[17600 rows x 81 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict8Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11ef48",
   "metadata": {},
   "source": [
    "Next, a `RandomForestClassifier` model will be trained on the created dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f700401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "\n",
    "rfc.fit(dict8Frame, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778330e",
   "metadata": {},
   "source": [
    "The test set will be vectorized and predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "55933584",
   "metadata": {},
   "outputs": [],
   "source": [
    "testVectorizor = feature_engineering(X_test, dict8_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f939dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_8pred = rfc.predict(testVectorizor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c728f",
   "metadata": {},
   "source": [
    "Now, to display our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3e3f7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of top 8 used characters: 0.8390909090909091\n",
      "Precision Score of top 8 used characters: 0.8390909090909091\n",
      "Recall Score of top 8 used characters: 0.8390909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "acc8 = accuracy_score(y_test, y_8pred)\n",
    "prec8 = precision_score(y_test, y_8pred, average='micro')\n",
    "rec8 = recall_score(y_test, y_8pred, average='micro')\n",
    "\n",
    "print(\"Accuracy Score of top 8 used characters:\",acc8)\n",
    "print(\"Precision Score of top 8 used characters:\",prec8)\n",
    "print(\"Recall Score of top 8 used characters:\",rec8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b372",
   "metadata": {},
   "source": [
    "Now for the Top 10 Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0480785",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict10Frame = feature_engineering(X_train, dict10_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ec729cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>م</th>\n",
       "      <th>ு</th>\n",
       "      <th>l</th>\n",
       "      <th>ر</th>\n",
       "      <th>त</th>\n",
       "      <th>و</th>\n",
       "      <th>ப</th>\n",
       "      <th>た</th>\n",
       "      <th>ม</th>\n",
       "      <th>в</th>\n",
       "      <th>...</th>\n",
       "      <th>ے</th>\n",
       "      <th>e</th>\n",
       "      <th>่</th>\n",
       "      <th>د</th>\n",
       "      <th>ー</th>\n",
       "      <th>ร</th>\n",
       "      <th>த</th>\n",
       "      <th>r</th>\n",
       "      <th>m</th>\n",
       "      <th>்</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090551</td>\n",
       "      <td>0.043307</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079585</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17595</th>\n",
       "      <td>0.040956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.051195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076233</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098485</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17598</th>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17600 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              م         ு         l         ر    त         و         ப  \\\n",
       "0      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "2      0.000000  0.000000  0.031496  0.000000  0.0  0.000000  0.000000   \n",
       "3      0.000000  0.044983  0.000000  0.000000  0.0  0.000000  0.048443   \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "17595  0.040956  0.000000  0.000000  0.061433  0.0  0.051195  0.000000   \n",
       "17596  0.000000  0.000000  0.040359  0.000000  0.0  0.000000  0.000000   \n",
       "17597  0.000000  0.000000  0.078283  0.000000  0.0  0.000000  0.000000   \n",
       "17598  0.017544  0.000000  0.000000  0.043860  0.0  0.061404  0.000000   \n",
       "17599  0.000000  0.000000  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "\n",
       "              た         ม         в  ...    ے         e         ่         د  \\\n",
       "0      0.000000  0.037838  0.000000  ...  0.0  0.000000  0.032432  0.000000   \n",
       "1      0.000000  0.028169  0.000000  ...  0.0  0.000000  0.042254  0.000000   \n",
       "2      0.000000  0.000000  0.000000  ...  0.0  0.106299  0.000000  0.000000   \n",
       "3      0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
       "4      0.000000  0.000000  0.052894  ...  0.0  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...  ...  ...       ...       ...       ...   \n",
       "17595  0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.027304   \n",
       "17596  0.000000  0.000000  0.000000  ...  0.0  0.085202  0.000000  0.000000   \n",
       "17597  0.000000  0.000000  0.000000  ...  0.0  0.101010  0.000000  0.000000   \n",
       "17598  0.000000  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.057018   \n",
       "17599  0.006536  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000   \n",
       "\n",
       "         ー         ร         த         r         m         ்  \n",
       "0      0.0  0.021622  0.000000  0.000000  0.000000  0.000000  \n",
       "1      0.0  0.039906  0.000000  0.000000  0.000000  0.000000  \n",
       "2      0.0  0.000000  0.000000  0.090551  0.043307  0.000000  \n",
       "3      0.0  0.000000  0.079585  0.003460  0.000000  0.148789  \n",
       "4      0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "...    ...       ...       ...       ...       ...       ...  \n",
       "17595  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "17596  0.0  0.000000  0.000000  0.076233  0.017937  0.000000  \n",
       "17597  0.0  0.000000  0.000000  0.098485  0.015152  0.000000  \n",
       "17598  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "17599  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[17600 rows x 104 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict10Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b8084b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, random_state=42)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "\n",
    "rfc2.fit(dict10Frame, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b9aaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2Vectorizor = feature_engineering(X_test, dict10_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b41d0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_10pred = rfc2.predict(test2Vectorizor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "463588d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of top 10 used characters: 0.8925\n",
      "Precision Score of top 10 used characters: 0.8925\n",
      "Recall Score of top 10 used characters: 0.8925\n"
     ]
    }
   ],
   "source": [
    "acc10 = accuracy_score(y_test, y_10pred)\n",
    "prec10 = precision_score(y_test, y_10pred, average='micro')\n",
    "rec10 = recall_score(y_test, y_10pred, average='micro')\n",
    "\n",
    "print(\"Accuracy Score of top 10 used characters:\",acc10)\n",
    "print(\"Precision Score of top 10 used characters:\",prec10)\n",
    "print(\"Recall Score of top 10 used characters:\",rec10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df183b17",
   "metadata": {},
   "source": [
    "Now Let's Use Some Cleaned Data to see if our accuracy increases\n",
    "\n",
    "_A custom data cleaner program was used to removed duplicate words from the dataset. So, if an Enlgish word was in the Chinese or Japanese set of words, for example, it would be removed from that section. The Pushto language was removed as it was causing errors during the cleaning process_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "99c233a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in our Cleaned Data\n",
    "data_clean = pd.read_csv(\"cleaned_data.csv\")\n",
    "\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "51f089c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data into Training and Testing Sets\n",
    "X_clean = data_clean['Text']\n",
    "y_clean = data_clean['language']\n",
    "\n",
    "\n",
    "language_list_clean = set(y_clean)\n",
    "\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7e73f12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic',\n",
       " 'Chinese',\n",
       " 'Dutch',\n",
       " 'English',\n",
       " 'Estonian',\n",
       " 'French',\n",
       " 'Hindi',\n",
       " 'Indonesian',\n",
       " 'Japanese',\n",
       " 'Korean',\n",
       " 'Latin',\n",
       " 'Persian',\n",
       " 'Portugese',\n",
       " 'Romanian',\n",
       " 'Russian',\n",
       " 'Spanish',\n",
       " 'Swedish',\n",
       " 'Tamil',\n",
       " 'Thai',\n",
       " 'Turkish',\n",
       " 'Urdu'}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5f0ac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Top 8 and 10 Char Dictionaries for Clean Data\n",
    "charVectorizor_clean = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_train_clean_raw = charVectorizor_clean.fit_transform(X_train_clean)\n",
    "X_test_clean_raw = charVectorizor_clean.transform(X_test_clean)\n",
    "language_dict_clean = train_lang_dict(X_train_clean_raw.toarray(), y_train_clean.values)\n",
    "\n",
    "Features_clean = charVectorizor_clean.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "75d29c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "top8PerLanguage_dict_clean = getRelevantGramsPerLanguage(Features_clean, language_dict_clean, language_list_clean, top=8)\n",
    "top10PerLanguage_dict_clean = getRelevantGramsPerLanguage(Features_clean, language_dict_clean, language_list_clean, top=10)\n",
    "\n",
    "dict8_array_clean = dictToArray(top8PerLanguage_dict_clean, language_list_clean)\n",
    "dict10_array_clean = dictToArray(top10PerLanguage_dict_clean, language_list_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e6357336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize Clean Data\n",
    "dict8Frame_clean = feature_engineering(X_train_clean, dict8_array_clean)\n",
    "dict10Frame_clean = feature_engineering(X_train_clean, dict10_array_clean)\n",
    "\n",
    "testVectorizor_clean = feature_engineering(X_test_clean, dict8_array_clean)\n",
    "test2Vectorizor_clean = feature_engineering(X_test_clean, dict10_array_clean)\n",
    "\n",
    "#Train Models for 8 and 10 Char\n",
    "rfc3 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rfc3.fit(dict8Frame_clean, y_train_clean)\n",
    "\n",
    "rfc4 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rfc4.fit(dict10Frame_clean, y_train_clean)\n",
    "\n",
    "#Make Predictions\n",
    "y_8pred_clean = rfc3.predict(testVectorizor_clean)\n",
    "y_10pred_clean = rfc4.predict(test2Vectorizor_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "834f10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for 8 Chars: 0.8054761904761905\n",
      "Precision Score for 8 Chars: 0.8054761904761905\n",
      "Recall Score for 8 Chars: 0.8054761904761905 \n",
      "\n",
      "Accuracy Score for 10 Chars: 0.8428571428571429\n",
      "Precision Score for 10 Chars: 0.8428571428571429\n",
      "Recall Score for 10 Chars: 0.8428571428571429\n"
     ]
    }
   ],
   "source": [
    "#Display Accuracy Score\n",
    "acc8_clean = accuracy_score(y_test_clean, y_8pred_clean)\n",
    "prec8_clean = precision_score(y_test_clean, y_8pred_clean, average='micro')\n",
    "rec8_clean = recall_score(y_test_clean, y_8pred_clean, average='micro')\n",
    "\n",
    "acc10_clean = accuracy_score(y_test_clean, y_10pred_clean)\n",
    "prec10_clean = precision_score(y_test_clean, y_10pred_clean, average='micro')\n",
    "rec10_clean = recall_score(y_test_clean, y_10pred_clean, average='micro')\n",
    "\n",
    "print(\"Accuracy Score for 8 Chars:\", acc8_clean)\n",
    "print(\"Precision Score for 8 Chars:\", prec8_clean)\n",
    "print(\"Recall Score for 8 Chars:\", rec8_clean, \"\\n\")\n",
    "\n",
    "print(\"Accuracy Score for 10 Chars:\", acc10_clean)\n",
    "print(\"Precision Score for 10 Chars:\", prec10_clean)\n",
    "print(\"Recall Score for 10 Chars:\", rec10_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a8492e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage Diffs:\n",
      " Accuracy 8 Char: -3.3614718614718653%\n",
      " Precision 8 Char: -3.3614718614718653%\n",
      " Recall 8 Char: -3.3614718614718653%\n",
      "Accuracy 10 Char: -4.96428571428571%\n",
      " Precision 10 Char: -4.96428571428571%\n",
      " Recall 10 Char: -4.96428571428571%\n"
     ]
    }
   ],
   "source": [
    "#Report Percentage Diff\n",
    "acc8_diff = (acc8_clean - acc8)*100\n",
    "prec8_diff = (prec8_clean - prec8)*100\n",
    "rec8_diff = (rec8_clean - rec8)*100\n",
    "\n",
    "acc10_diff = (acc10_clean - acc10)*100\n",
    "prec10_diff = (prec10_clean - prec10)*100\n",
    "rec10_diff = (rec10_clean - rec10)*100\n",
    "\n",
    "print(\"Precentage Diffs:\\n Accuracy 8 Char: {}%\\n Precision 8 Char: {}%\\n Recall 8 Char: {}%\\nAccuracy 10 Char: {}%\\n Precision 10 Char: {}%\\n Recall 10 Char: {}%\".format(acc8_diff, prec8_diff, rec8_diff, acc10_diff, prec10_diff, rec10_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b05f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these arrays into dictonaries\n",
    "english_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "estonian_alpha = [A, B, D, E, F, G, H, I, J, K, L, M, N, O, P, R, S, Š, Z, Ž, T, U, V, Õ, Ä, Ö, Ü]\n",
    "swedish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, å, ä, ö]\n",
    "thai_alpha = [ก, ข, ค, ฅ, ฆ, ง, จ, ฉ, ช, ฌ, ญ, ฎ, ฏ, ฐ, ฑ, ฒ, ณ, ด, ต, ถ, ท, ธ, น, บ, บ, ผ, ฝ, พ, ฟ, ภ, \n",
    "               ม, ย, ร, ล, ว, ศ, ษ, ส, ห, ฬ, อ, ฮ] \n",
    "tamil_alpha = [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ங, ச, ஞ, ட, ண, த, ந, ன, ப, ம, ய, ர, ற, ல, ள, ழ, வ]\n",
    "dutch_alpha = english_alpha\n",
    "japanese_alpha = [ぁ, あ, ぃ, い, ぅ, う, ぇ, え, ぉ, お, か, が, き, ぎ, く, ぐ, け, げ, こ, ご, さ, ざ, し, じ, す, ず,\n",
    "                  せ, ぜ, そ, ぞ, た, だ, ち, ぢ, っ, つ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ,\n",
    "                  ふ, ぶ, ぷ, へ, べ, れ, る, り, ら, よ, ょ, ゆ, ゅ, や, ゃ, も, め, む, み, ま, ぽ, ぼ, ほ, ぺ, ろ, ゎ,\n",
    "                  わ, ゐ, ゑ, を, ん, ゔ, ゕ, ゖ,  ゚, ゛, ゜, ゝ, ゞ, ゟ, ゠, ァ, ア, サ, ゴ, コ, ゲ, ケ, グ, ク, ギ, キ,\n",
    "                  ガ, カ, オ, ォ, エ, ェ, ウ, ゥ, イ, ィ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ ,ダ ,チ ,ヂ, ッ, ツ, ヅ,\n",
    "                  テ, デ, ト, ホ, ペ, ベ, ヘ, プ, ブ, フ, ピ, ビ, ヒ, パ, バ, ハ, ノ, ネ, ヌ, ニ, ナ, ド, ボ, ポ, マ, ミ, \n",
    "                  ム, メ, モ, ャ, ヤ, ュ, ユ, ョ, ヨ, ラ, リ, ル, レ, ロ, ヮ, ㍿, ㍐, ヿ, ヾ, ヽ, ー, ・, ヺ, ヹ, ヸ, ヷ,\n",
    "                  ヶ, ヵ, ヴ, ン, ヲ, ヱ, ヰ, ワ]\n",
    "turkish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, r, s, t, u, v, y, z, ç, ğ, ı, İ, î, ö, ş, ü]\n",
    "latin_alpha = english_alpha\n",
    "urdu_alpha = [ش,س,ژ,ز,ڑ,ر,ذ,ڈ,د,خ,ح,چ,\n",
    "              ج,ث,ٹ,ت,پ,ب,آ,ا,ے,ی,ھ,ہ,و,ں,ن,م,ل,گ,ک,ق,ف,غ,ع,ظ,ط,ض,ص]\n",
    "indonesian_alpha = english_alpha\n",
    "portuguese_alpha = [ç, á, é, í, ó, ú, â, ê, ô, ã, õ, à, è, ì, ò, ù, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "french_alpha = [ç, é, â, ê, î, ô, û, à, è, ì, ò, ù, ë, ï, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "chinese_alpha = [胡, 赛, 尼, 本, 人, 和, 小, 说, 的, 主, 人, 公, 阿, 米, 尔, 一, 样, 都, 是, 出, 生, 在, 阿, 富, 汗, 首, 都, \n",
    "                 喀, 布, 尔, 少, 年, 时, 代, 便, 离, 开, 了, 这, 个, 国, 家, 。, 胡, 赛, 尼, 直, 到, 年, 小, 说, 出, 版, 之, \n",
    "                 后, 才, 首, 次, 回, 到, 已, 经, 离, 开, 年, 的, 祖, 国, 。, 他, 在, 苏, 联, 入, 侵, 时, 离, 开, 了, 阿, 富, \n",
    "                 汗, 而, 他, 的, 很, 多, 童, 年, 好, 友, 在, 阿, 富, 汗, 生, 活, 在, 他, 们, 出, 发, 之, 前, 罗, 伯, 特, 伊,\n",
    "                 达, 尔, 文, 卷, 查, 尔, 斯, 赖, 尔, 所, 著, 地, 质, 学, 原, 理, 在, 南, 美, 他, 得, 到, 第, 卷, 该, 书, 将, \n",
    "                 地, 形, 地, 貌, 解, 释, 为, 漫, 长, 历, 史, 时, 间, 渐, 进, 演, 变, 的, 的, 结, 果, 当, 他, 旅, 程, 的, 第, \n",
    "                 站, 抵, 达, 圣, 地, 亚, 哥, 佛, 得, 角, 的, 时, 候, 达, 尔, 文]\n",
    "korean_alpha = [ᄁ,ᄂ,ᄃ,ᄄ,ᄅᄆᄇ,ᄈ,ᄉ,ᄊ,ᄋ,ᄌᄍ,ᄎ,ᄏ,ᄐ,ᄑᄒ,아,악,안,알,암,압,앙,앞애,액,앵야,얀,약,양,얘,어,억,\n",
    "                언,얼,엄,업,엉,에,여,역,연,열,염,엽,영,예,ᄀ,여,역,연,열,염,엽,영,예,오,옥,온,올,옴,옹,와,완,왈,왕,왜,외,왼,\n",
    "                요,욕,용,우,욱,운,울,움,웅,워,원,월,위,유,육,윤,율,융,윷,으,은,을,음읍,응,의,이,익,인,일,임,입,잉,잎]\n",
    "hindi_alpha = [ऄ, अ, आ, इ, ई, उ, ऊ, ऋ, ऌ, ऍ, ऎ, ए, ऐ, ऑ, ऒ, ओ, औ, क, ख, ग, घ, ङ, च, छ, ज, झ, प, ऩ, न, ध, द, \n",
    "               थ, त, ण, ढ, ड, ठ, ट, ञ, फ, ब, भ, म, य, र, ऱ, ल, ळ, ऴ, व, श, ष, ४, ३, २, १, ०, ॥, ।, ॡ, ॠ, ॐ, ऽ, \n",
    "               ह, स, ५, ६, ७, ॲ, ॳ, ॴ, ॵ, ॶ, ॷ, ॹ, ॺ, ॻ, ॼ, ॾ, ॿ, ೱऀँं, ः, ऺ, ऻ, ा, ि, ी, ॎ, ॏॕैेॣॢ, ॗ]\n",
    "spanish_alpha = [á, é, í, ó, ú, ñ, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "pushto_alpha = [,ﺏ ,پ ,ﺕ ,ټ ,ﺙ ,ﺝ ,چ ,ﺡ ,ﺥ ,څ ,ځ ,ﺩ ,ډ ,ﺫ ,ﺭ ,ړ ,ﺯ ,ژ ,ږ ,ﺱ ,ﺵ ,ښ ,ﺹ ,ﺽ ,ﻁ ,ﻅ ,ﻉ ,ﻍ ,ﻑ ,ﻕ ,ک ,ګ ,ﻝ ,ﻡ ,ﻥ ,ڼ, ,ﻭ ,ه ,ۀ ,ي ,ې ,ی ,ۍ ,ئ]\n",
    "persian_alpha = [,ش,س,ژ,ز,ر,ذ,د,خ,ح,چ,ج,ث,ت,پ,ب,آ,ا,ص,ض,ط,ظ,ع,غ,ف,ق,ک,گ,ل,م,ن,و,ه,ی]\n",
    "romanian_alpha = [ă, â, î, ș, ş, ț, ţ, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "russian_alpha = [б, в, г, д, ж, з, к, л, м, н, п, р, с, т, ф, х, ц, ч, ш, щ, а, е, ё, и, о, у, ы, э, ю, я, й]\n",
    "arabic_alpha = [ش,س,ز,ر,ذ,د,خ,ح,ج,ث,ت,ب,ا,ء,ي,و,ه,ن,م,ل,ك,ق,ف,غ,ع,ظ,ط,ض,ص]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Average Word Length of a single string\n",
    "def avg_word_len(string):\n",
    "    # Split string up and find total amount of words present\n",
    "    words = string.split()\n",
    "    wordCount = len(words)\n",
    "    \n",
    "    # Calculate actual average  \n",
    "    ch = 0\n",
    "    for word in words:\n",
    "        ch += len(word) # Add up all chars\n",
    "    avg = ch / wordCount # Divide sum of chars by amount of words present\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6809f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Average Sentence Length across a piece of text\n",
    "def avg_sent_len(text):\n",
    "  sentences = text.split(\".\") # Split the text into a list of sentences.\n",
    "  words = text.split(\" \") # split the input text into a list of separate words\n",
    "  if(sentences[len(sentences)-1]==\"\"): # if the last value in sentences is an empty string\n",
    "    average_sentence_length = len(words) / len(sentences)-1\n",
    "  else:\n",
    "    average_sentence_length = len(words) / len(sentences)\n",
    "  return average_sentence_length # returning avg length of sentence\n",
    "  \n",
    "ans = avg_sent_len(\"I am going.to see you later\") # function call\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test this function (avg_sent_len)          #Mr., Dr., Ms., etc. are words that may be a problem for this function\n",
    "avg_sent_len(input(\"Provide a body of text: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "features = pd.get_dummies(data_trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
