{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd55be",
   "metadata": {},
   "source": [
    "# Understanding the Kaggle Data\n",
    "The dataset acquired from [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/data) contains words from several different languages. The noise contained in the dataset are duplicate words. To reduce this noise, the words will be broken down into single and double characters, then rated based on how often they show up in that respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data*\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18753df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "language_list = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017c263d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "                               ...                        \n",
       "11964    باباجان غفورف تاریخ‌دان و نویسندهٔ کتاب تاریخ ...\n",
       "21575    en  fue invitado por fernando ii para ocupar l...\n",
       "5390     doğu kanada atabasklarına geleneksel olarak dü...\n",
       "860      پژواک د يوې ځانگړې پروژې په توگه د اساسي قانون...\n",
       "15795    テンサイについては糖分を高度に精製する必要があることからサトウキビと同じような黒糖を作るのは...\n",
       "Name: Text, Length: 17600, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            \n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "            \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "276f6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_into_dataframe(array, chars):\n",
    "    data_frame = pd.DataFrame(array, columns = chars)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5739d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5207     สัมประสิทธิ์ฮอลล์ ฟิสิกส์ไฟฟ้า เกี่ยวกับสนามแม...\n",
       "4450     เกิดวันที่  พฤศจิกายน ภาคอะนิเมะ คดีฆาตกรรมบนจ...\n",
       "7033     i omgivningarna runt manigotagan river park re...\n",
       "487      நிஞ்சா ஹட்டோரி 忍者ハットリくん ninja hattori என்பது க...\n",
       "19537    эта страница деятельности м в ломоносова — ярк...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda = X_train.head()\n",
    "panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce97ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_22_features = ['e', 't', 'a', 'i', 'o', 'á', 'é', 'í']\n",
    "\n",
    "test_data = feature_engineering(panda, first_22_features)\n",
    "test_data_2 = numpy_into_dataframe(test_data, first_22_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa4cb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>t</th>\n",
       "      <th>a</th>\n",
       "      <th>i</th>\n",
       "      <th>o</th>\n",
       "      <th>á</th>\n",
       "      <th>é</th>\n",
       "      <th>í</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.15748</td>\n",
       "      <td>0.244094</td>\n",
       "      <td>0.295276</td>\n",
       "      <td>0.318898</td>\n",
       "      <td>0.318898</td>\n",
       "      <td>0.318898</td>\n",
       "      <td>0.318898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00692</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.024221</td>\n",
       "      <td>0.024221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          e        t         a         i         o         á         é  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.106299  0.15748  0.244094  0.295276  0.318898  0.318898  0.318898   \n",
       "3  0.000000  0.00692  0.013841  0.020761  0.024221  0.024221  0.024221   \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          í  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.318898  \n",
       "3  0.024221  \n",
       "4  0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2a28",
   "metadata": {},
   "source": [
    "## Creating Features: Most Used Characters\n",
    "To train the models to determine which language is being used by the user, we first need to know which characters are used in each language. The best approach for this is to used the dataset, _which is already using the characters from each language_, and find the **most used characters** in them.\n",
    "\n",
    "### Kaggle Resources\n",
    "----\n",
    "The following code is taken from the [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/notebook) to understand how to process characters for each language. Kaggle uses the `CountVectorizor` Method from the `sklearn` module to tokenize the characters into readable 1's and 0's. Then, it counts how many times that character has been used in the sample data provided.\n",
    "\n",
    "This method reduces the necessity of locating alphabets for each language and creating custom functions to find the most used characters in the dataset.\n",
    " \n",
    "**To summarize Kaggle's findings: Languages based off the Latin Alphabet are easier to differentiate from each other in the data set while Languages with their own Alphabet, _like Chinese and Japanese_, can be differentiated by single characters alone.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3797753",
   "metadata": {},
   "source": [
    "This code here fits and transforms the X_train and X_test data sets into readable 1's and 0's and counts the number of times a specific character shows up in the datasets.The `min_df` parameter tells `CountVectorizor` to save any characters that are used **at least 1% of the time** in the dataset.\n",
    "\n",
    "These matrices are saved as `X_top1Percent_train_raw` and `X_top1Percent_test_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb378224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a list of single and double characters from the top 1% of to be used as features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b0c14",
   "metadata": {},
   "source": [
    "The `train_lang_dict()` function takes in the raw vectorized X_train and y_train data sets and converts them into readable dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Command from Kaggle connects the character features to their specific language\n",
    "\n",
    "# Aggregate Unigrams per language\n",
    "def train_lang_dict(X_raw_counts, y_train):\n",
    "    lang_dict = {}\n",
    "    for i in range(len(y_train)):\n",
    "        lang = y_train[i]\n",
    "        v = np.array(X_raw_counts[i])\n",
    "        if not lang in lang_dict:\n",
    "            lang_dict[lang] = v\n",
    "        else:\n",
    "            lang_dict[lang] += v\n",
    "            \n",
    "    # to relative\n",
    "    for lang in lang_dict:\n",
    "        v = lang_dict[lang]\n",
    "        lang_dict[lang] = v / np.sum(v)\n",
    "        \n",
    "    return lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b62b41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n",
    "\n",
    "top1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f036f3",
   "metadata": {},
   "source": [
    "The `getRelevantGramsPerLanguage()` function processes the dictionary and returns a dictionary with only the top 50 **most used** characters for **each** language. This number _can_ be changed by setting `top=x` when you call `getRelevantGramsPerLanguage()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e18e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantGramsPerLanguage(features, language_dict, top=50):\n",
    "    relevantGramsPerLanguage = {}\n",
    "    for lang in language_list:\n",
    "        chars = []\n",
    "        relevantGramsPerLanguage[lang] = chars\n",
    "        v = language_dict[lang]\n",
    "        sortIndex = (-v).argsort()[:top]\n",
    "        for i in range(len(sortIndex)):\n",
    "            chars.append(features[sortIndex[i]])\n",
    "    return relevantGramsPerLanguage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae2d19",
   "metadata": {},
   "source": [
    "Below Displays the top 8 and 10 Characters from each language in a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a571530d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と'],\n",
       " 'Indonesian': ['a', ' ', 'n', 'e', 'i', 'r', 'u', 't'],\n",
       " 'Latin': [' ', 'i', 'a', 'e', 't', 's', 'n', 'r'],\n",
       " 'Estonian': [' ', 'a', 'i', 'e', 's', 't', 'l', 'n'],\n",
       " 'Thai': ['า', 'น', 'ร', ' ', 'ก', 'อ', '่', 'เ'],\n",
       " 'French': [' ', 'e', 'a', 'n', 's', 'i', 'r', 't'],\n",
       " 'Russian': [' ', 'о', 'и', 'е', 'а', 'н', 'с', 'р'],\n",
       " 'Swedish': [' ', 'e', 'r', 'a', 'n', 't', 'i', 's'],\n",
       " 'Chinese': ['，', '的', '。', ' ', '年', '在', '、', '一'],\n",
       " 'Portugese': [' ', 'a', 'e', 'o', 's', 'i', 'r', 'd'],\n",
       " 'Romanian': [' ', 'e', 'a', 'i', 'r', 'n', 't', 'u'],\n",
       " 'Persian': [' ', 'ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و'],\n",
       " 'Arabic': [' ', 'ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا'],\n",
       " 'Tamil': ['்', ' ', 'க', 'ு', 'ி', 'த', '் ', 'ப'],\n",
       " 'Pushto': [' ', 'و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر'],\n",
       " 'Dutch': [' ', 'e', 'n', 'a', 'i', 'r', 't', 'o'],\n",
       " 'Turkish': [' ', 'a', 'e', 'i', 'n', 'r', 'l', 'ı'],\n",
       " 'Korean': [' ', '이', '의', '다', '의 ', '에', '는', '는 '],\n",
       " 'Spanish': [' ', 'e', 'a', 'o', 'n', 's', 'r', 'i'],\n",
       " 'Hindi': [' ', 'ा', 'क', 'र', '्', 'े', 'ि', 'स'],\n",
       " 'English': [' ', 'e', 'a', 't', 'i', 'o', 'n', 's'],\n",
       " 'Urdu': [' ', 'ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top8PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=8)\n",
    "top8PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01d01a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Japanese': ['の', '、', 'に', 'た', 'る', '。', 'は', 'と', 'ー', 'を'],\n",
       " 'Indonesian': ['a', ' ', 'n', 'e', 'i', 'r', 'u', 't', 's', 'an'],\n",
       " 'Latin': [' ', 'i', 'a', 'e', 't', 's', 'n', 'r', 'u', 'o'],\n",
       " 'Estonian': [' ', 'a', 'i', 'e', 's', 't', 'l', 'n', 'u', 'o'],\n",
       " 'Thai': ['า', 'น', 'ร', ' ', 'ก', 'อ', '่', 'เ', 'ง', 'ม'],\n",
       " 'French': [' ', 'e', 'a', 'n', 's', 'i', 'r', 't', 'l', 'e '],\n",
       " 'Russian': [' ', 'о', 'и', 'е', 'а', 'н', 'с', 'р', 'т', 'в'],\n",
       " 'Swedish': [' ', 'e', 'r', 'a', 'n', 't', 'i', 's', 'd', 'l'],\n",
       " 'Chinese': ['，', '的', '。', ' ', '年', '在', '、', '一', '中', 'a'],\n",
       " 'Portugese': [' ', 'a', 'e', 'o', 's', 'i', 'r', 'd', 'n', 't'],\n",
       " 'Romanian': [' ', 'e', 'a', 'i', 'r', 'n', 't', 'u', 'l', 'o'],\n",
       " 'Persian': [' ', 'ا', 'ی', 'ر', 'د', 'ن', 'ه', 'و', 'م', 'ت'],\n",
       " 'Arabic': [' ', 'ا', 'ل', 'ي', 'ال', 'م', 'و', ' ا', 'ن', 'ت'],\n",
       " 'Tamil': ['்', ' ', 'க', 'ு', 'ி', 'த', '் ', 'ப', 'ம', 'ட'],\n",
       " 'Pushto': [' ', 'و', 'ا', 'ه', 'ي', 'ه ', 'د', 'ر', 'ل', 'ن'],\n",
       " 'Dutch': [' ', 'e', 'n', 'a', 'i', 'r', 't', 'o', 'd', 's'],\n",
       " 'Turkish': [' ', 'a', 'e', 'i', 'n', 'r', 'l', 'ı', 'd', 'k'],\n",
       " 'Korean': [' ', '이', '의', '다', '의 ', '에', '는', '는 ', '하', '을'],\n",
       " 'Spanish': [' ', 'e', 'a', 'o', 'n', 's', 'r', 'i', 'l', 'd'],\n",
       " 'Hindi': [' ', 'ा', 'क', 'र', '्', 'े', 'ि', 'स', ' क', 'न'],\n",
       " 'English': [' ', 'e', 'a', 't', 'i', 'o', 'n', 's', 'r', 'h'],\n",
       " 'Urdu': [' ', 'ا', 'ی', 'ر', 'و', 'ک', 'م', 'ن', 'ہ', 'ے']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent, top=10)\n",
    "top10PerLanguage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaaf4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictToArray(dict, languages=language_list):\n",
    "    '''Converts a Language dictionary to an array and removes the duplicate values'''\n",
    "    char = []\n",
    "\n",
    "    for lang in languages:\n",
    "        arr = dict[lang]\n",
    "        char = char + arr\n",
    "    \n",
    "    dict_array = list(set(char))\n",
    "\n",
    "    return dict_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1263e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'ه ', '் ', 'l', 'р', 'م', 'e', 'น', 'ி', 'ப', 'の', 'n', 'ل', 'は', ' ', 'н', 'ن', '年', 'த', 'u', 'ı', 'と', 'о', '이', '는', 'ก', '்', 'க', 'd', 'า', 'د', 'o', '्', '의', 'е', 'ร', 'и', '在', ' ا', '는 ', 'स', '의 ', 'ي', 'े', 's', 'а', '่', 'с', '、', 'ه', '。', 'ر', '다', 'に', 'و', 'i', 'ு', '에', 'र', 'ک', '，', '一', 'ی', 'ि', 'r', 'ा', 'た', '的', 'เ', 'क', 'ال', 'อ', 'ا', 'る', 'a']\n",
      "Total Number of Characters: 75\n"
     ]
    }
   ],
   "source": [
    "dict8_array = dictToArray(top8PerLanguage_dict)\n",
    "\n",
    "print(dict8_array)\n",
    "print(\"Total Number of Characters:\", len(dict8_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c832284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'ி', 'ப', 'ل', 'は', 'ن', 'த', '年', '는', 'd', 'า', 'د', 'स', 'े', 'ு', 'h', 'r', 'た', 'เ', '을', 'อ', 'a', 'م', 'ت', 'ı', ' ', 'н', '이', 'u', 'と', 'о', 'ก', '்', 'க', '의', 'е', '의 ', 's', '่', 'ه', 'र', '。', '다', 'に', 'و', '에', 'ง', '，', '一', 'ि', 'ा', '하', 'k', ' क', 'ا', 'ー', 't', 'ه ', 'р', 'e', 'の', 'e ', 'n', '는 ', '्', 'ร', 'を', '在', 'क', 'ر', 'ک', 'ม', 'ہ', 'ے', 'т', '中', '் ', 'l', 'น', 'न', 'o', 'и', ' ا', 'ي', 'а', 'в', 'с', '、', 'i', 'ی', 'ம', '的', 'ட', 'ال', 'る']\n",
      "Total Number of Characters: 95\n"
     ]
    }
   ],
   "source": [
    "dict10_array = dictToArray(top10PerLanguage_dict)\n",
    "\n",
    "print(dict10_array)\n",
    "print(\"Total Number of Characters:\", len(dict10_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58581a6a",
   "metadata": {},
   "source": [
    "## Training the Model: RandomForests\n",
    "Now that we know the top 8 and 10 used characters for each language, we can use them to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332a707",
   "metadata": {},
   "source": [
    "First, we'll create a `CountVectorizor` instance to read in the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0d27435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictVectorizor = CountVectorizer(analyzer='char', ngram_range=(1,2))\n",
    "\n",
    "X_top8 = dictVectorizor.fit_transform(dict8_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f700401",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [75, 4400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/giovanni/Dev-Any/Development/CSCI_405/Group_Project/Language_Detector.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/giovanni/Dev-Any/Development/CSCI_405/Group_Project/Language_Detector.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/giovanni/Dev-Any/Development/CSCI_405/Group_Project/Language_Detector.ipynb#ch0000026?line=2'>3</a>\u001b[0m rfc \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/giovanni/Dev-Any/Development/CSCI_405/Group_Project/Language_Detector.ipynb#ch0000026?line=4'>5</a>\u001b[0m rfc\u001b[39m.\u001b[39;49mfit(X_top8, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=326'>327</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=327'>328</a>\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=328'>329</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=329'>330</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=330'>331</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/base.py?line=578'>579</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/base.py?line=579'>580</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/base.py?line=580'>581</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/base.py?line=581'>582</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/base.py?line=583'>584</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=963'>964</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=964'>965</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=965'>966</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=975'>976</a>\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=976'>977</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=978'>979</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m--> <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=980'>981</a>\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=982'>983</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=329'>330</a>\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=330'>331</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=331'>332</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=332'>333</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=333'>334</a>\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    <a href='file:///home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/validation.py?line=334'>335</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [75, 4400]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_top8, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_top1Percent_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of top 1 percent used characters: 0.9802272727272727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy Score of top 1 percent used characters:\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b05f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these arrays into dictonaries\n",
    "english_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "estonian_alpha = [A, B, D, E, F, G, H, I, J, K, L, M, N, O, P, R, S, Š, Z, Ž, T, U, V, Õ, Ä, Ö, Ü]\n",
    "swedish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, å, ä, ö]\n",
    "thai_alpha = [ก, ข, ค, ฅ, ฆ, ง, จ, ฉ, ช, ฌ, ญ, ฎ, ฏ, ฐ, ฑ, ฒ, ณ, ด, ต, ถ, ท, ธ, น, บ, บ, ผ, ฝ, พ, ฟ, ภ, \n",
    "               ม, ย, ร, ล, ว, ศ, ษ, ส, ห, ฬ, อ, ฮ] \n",
    "tamil_alpha = [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ங, ச, ஞ, ட, ண, த, ந, ன, ப, ம, ய, ர, ற, ல, ள, ழ, வ]\n",
    "dutch_alpha = english_alpha\n",
    "japanese_alpha = [ぁ, あ, ぃ, い, ぅ, う, ぇ, え, ぉ, お, か, が, き, ぎ, く, ぐ, け, げ, こ, ご, さ, ざ, し, じ, す, ず,\n",
    "                  せ, ぜ, そ, ぞ, た, だ, ち, ぢ, っ, つ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ,\n",
    "                  ふ, ぶ, ぷ, へ, べ, れ, る, り, ら, よ, ょ, ゆ, ゅ, や, ゃ, も, め, む, み, ま, ぽ, ぼ, ほ, ぺ, ろ, ゎ,\n",
    "                  わ, ゐ, ゑ, を, ん, ゔ, ゕ, ゖ,  ゚, ゛, ゜, ゝ, ゞ, ゟ, ゠, ァ, ア, サ, ゴ, コ, ゲ, ケ, グ, ク, ギ, キ,\n",
    "                  ガ, カ, オ, ォ, エ, ェ, ウ, ゥ, イ, ィ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ ,ダ ,チ ,ヂ, ッ, ツ, ヅ,\n",
    "                  テ, デ, ト, ホ, ペ, ベ, ヘ, プ, ブ, フ, ピ, ビ, ヒ, パ, バ, ハ, ノ, ネ, ヌ, ニ, ナ, ド, ボ, ポ, マ, ミ, \n",
    "                  ム, メ, モ, ャ, ヤ, ュ, ユ, ョ, ヨ, ラ, リ, ル, レ, ロ, ヮ, ㍿, ㍐, ヿ, ヾ, ヽ, ー, ・, ヺ, ヹ, ヸ, ヷ,\n",
    "                  ヶ, ヵ, ヴ, ン, ヲ, ヱ, ヰ, ワ]\n",
    "turkish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, r, s, t, u, v, y, z, ç, ğ, ı, İ, î, ö, ş, ü]\n",
    "latin_alpha = english_alpha\n",
    "urdu_alpha = [ش,س,ژ,ز,ڑ,ر,ذ,ڈ,د,خ,ح,چ,\n",
    "              ج,ث,ٹ,ت,پ,ب,آ,ا,ے,ی,ھ,ہ,و,ں,ن,م,ل,گ,ک,ق,ف,غ,ع,ظ,ط,ض,ص]\n",
    "indonesian_alpha = english_alpha\n",
    "portuguese_alpha = [ç, á, é, í, ó, ú, â, ê, ô, ã, õ, à, è, ì, ò, ù, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "french_alpha = [ç, é, â, ê, î, ô, û, à, è, ì, ò, ù, ë, ï, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "chinese_alpha = [胡, 赛, 尼, 本, 人, 和, 小, 说, 的, 主, 人, 公, 阿, 米, 尔, 一, 样, 都, 是, 出, 生, 在, 阿, 富, 汗, 首, 都, \n",
    "                 喀, 布, 尔, 少, 年, 时, 代, 便, 离, 开, 了, 这, 个, 国, 家, 。, 胡, 赛, 尼, 直, 到, 年, 小, 说, 出, 版, 之, \n",
    "                 后, 才, 首, 次, 回, 到, 已, 经, 离, 开, 年, 的, 祖, 国, 。, 他, 在, 苏, 联, 入, 侵, 时, 离, 开, 了, 阿, 富, \n",
    "                 汗, 而, 他, 的, 很, 多, 童, 年, 好, 友, 在, 阿, 富, 汗, 生, 活, 在, 他, 们, 出, 发, 之, 前, 罗, 伯, 特, 伊,\n",
    "                 达, 尔, 文, 卷, 查, 尔, 斯, 赖, 尔, 所, 著, 地, 质, 学, 原, 理, 在, 南, 美, 他, 得, 到, 第, 卷, 该, 书, 将, \n",
    "                 地, 形, 地, 貌, 解, 释, 为, 漫, 长, 历, 史, 时, 间, 渐, 进, 演, 变, 的, 的, 结, 果, 当, 他, 旅, 程, 的, 第, \n",
    "                 站, 抵, 达, 圣, 地, 亚, 哥, 佛, 得, 角, 的, 时, 候, 达, 尔, 文]\n",
    "korean_alpha = [ᄁ,ᄂ,ᄃ,ᄄ,ᄅᄆᄇ,ᄈ,ᄉ,ᄊ,ᄋ,ᄌᄍ,ᄎ,ᄏ,ᄐ,ᄑᄒ,아,악,안,알,암,압,앙,앞애,액,앵야,얀,약,양,얘,어,억,\n",
    "                언,얼,엄,업,엉,에,여,역,연,열,염,엽,영,예,ᄀ,여,역,연,열,염,엽,영,예,오,옥,온,올,옴,옹,와,완,왈,왕,왜,외,왼,\n",
    "                요,욕,용,우,욱,운,울,움,웅,워,원,월,위,유,육,윤,율,융,윷,으,은,을,음읍,응,의,이,익,인,일,임,입,잉,잎]\n",
    "hindi_alpha = [ऄ, अ, आ, इ, ई, उ, ऊ, ऋ, ऌ, ऍ, ऎ, ए, ऐ, ऑ, ऒ, ओ, औ, क, ख, ग, घ, ङ, च, छ, ज, झ, प, ऩ, न, ध, द, \n",
    "               थ, त, ण, ढ, ड, ठ, ट, ञ, फ, ब, भ, म, य, र, ऱ, ल, ळ, ऴ, व, श, ष, ४, ३, २, १, ०, ॥, ।, ॡ, ॠ, ॐ, ऽ, \n",
    "               ह, स, ५, ६, ७, ॲ, ॳ, ॴ, ॵ, ॶ, ॷ, ॹ, ॺ, ॻ, ॼ, ॾ, ॿ, ೱऀँं, ः, ऺ, ऻ, ा, ि, ी, ॎ, ॏॕैेॣॢ, ॗ]\n",
    "spanish_alpha = [á, é, í, ó, ú, ñ, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "pushto_alpha = [,ﺏ ,پ ,ﺕ ,ټ ,ﺙ ,ﺝ ,چ ,ﺡ ,ﺥ ,څ ,ځ ,ﺩ ,ډ ,ﺫ ,ﺭ ,ړ ,ﺯ ,ژ ,ږ ,ﺱ ,ﺵ ,ښ ,ﺹ ,ﺽ ,ﻁ ,ﻅ ,ﻉ ,ﻍ ,ﻑ ,ﻕ ,ک ,ګ ,ﻝ ,ﻡ ,ﻥ ,ڼ, ,ﻭ ,ه ,ۀ ,ي ,ې ,ی ,ۍ ,ئ]\n",
    "persian_alpha = [,ش,س,ژ,ز,ر,ذ,د,خ,ح,چ,ج,ث,ت,پ,ب,آ,ا,ص,ض,ط,ظ,ع,غ,ف,ق,ک,گ,ل,م,ن,و,ه,ی]\n",
    "romanian_alpha = [ă, â, î, ș, ş, ț, ţ, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "russian_alpha = [б, в, г, д, ж, з, к, л, м, н, п, р, с, т, ф, х, ц, ч, ш, щ, а, е, ё, и, о, у, ы, э, ю, я, й]\n",
    "arabic_alpha = [ش,س,ز,ر,ذ,د,خ,ح,ج,ث,ت,ب,ا,ء,ي,و,ه,ن,م,ل,ك,ق,ف,غ,ع,ظ,ط,ض,ص]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Average Word Length of a single string\n",
    "def avg_word_len(string):\n",
    "    # Split string up and find total amount of words present\n",
    "    words = string.split()\n",
    "    wordCount = len(words)\n",
    "    \n",
    "    # Calculate actual average  \n",
    "    ch = 0\n",
    "    for word in words:\n",
    "        ch += len(word) # Add up all chars\n",
    "    avg = ch / wordCount # Divide sum of chars by amount of words present\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6809f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Average Sentence Length across a piece of text\n",
    "def avg_sent_len(text):\n",
    "  sentences = text.split(\".\") # Split the text into a list of sentences.\n",
    "  words = text.split(\" \") # split the input text into a list of separate words\n",
    "  if(sentences[len(sentences)-1]==\"\"): # if the last value in sentences is an empty string\n",
    "    average_sentence_length = len(words) / len(sentences)-1\n",
    "  else:\n",
    "    average_sentence_length = len(words) / len(sentences)\n",
    "  return average_sentence_length # returning avg length of sentence\n",
    "  \n",
    "ans = avg_sent_len(\"I am going.to see you later\") # function call\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13fa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets test this function (avg_sent_len)          #Mr., Dr., Ms., etc. are words that may be a problem for this function\n",
    "avg_sent_len(input(\"Provide a body of text: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b7e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encoding                                #Attempt failed lol\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder() # instantiate one hot encoder\n",
    "cat_encoder2 = cat_encoder.fit_transform(data_trans)\n",
    "cat_encoder2 #sparse matrix\n",
    "\n",
    "cat_encoder2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "features = pd.get_dummies(data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c881dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_ أ</th>\n",
       "      <th>1_ ہ</th>\n",
       "      <th>2_या</th>\n",
       "      <th>3_ د</th>\n",
       "      <th>4_行</th>\n",
       "      <th>5_》</th>\n",
       "      <th>6_こ</th>\n",
       "      <th>7_ng</th>\n",
       "      <th>8_े</th>\n",
       "      <th>9_अ</th>\n",
       "      <th>...</th>\n",
       "      <th>554_ட</th>\n",
       "      <th>555_்க</th>\n",
       "      <th>556_ा</th>\n",
       "      <th>557_서</th>\n",
       "      <th>558_을</th>\n",
       "      <th>559_த்</th>\n",
       "      <th>560_ु</th>\n",
       "      <th>561_t</th>\n",
       "      <th>562_も</th>\n",
       "      <th>563_ं</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chars</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0_ أ  1_ ہ  2_या  3_ د  4_行  5_》  6_こ  7_ng  8_े  9_अ  ...  554_ட  \\\n",
       "Chars     1     1     1     1    1    1    1     1    1    1  ...      1   \n",
       "\n",
       "       555_்க  556_ा   557_서   558_을   559_த்  560_ु  561_t  562_も  563_ं   \n",
       "Chars       1       1       1       1       1      1      1      1       1  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Chars</th>\n",
       "      <td>ed</td>\n",
       "      <td>た。</td>\n",
       "      <td>m</td>\n",
       "      <td>ی</td>\n",
       "      <td>nd</td>\n",
       "      <td>d</td>\n",
       "      <td>と</td>\n",
       "      <td>د</td>\n",
       "      <td>ار</td>\n",
       "      <td>和</td>\n",
       "      <td>...</td>\n",
       "      <td>ใ</td>\n",
       "      <td>ไ</td>\n",
       "      <td>h</td>\n",
       "      <td>d</td>\n",
       "      <td>่</td>\n",
       "      <td>क</td>\n",
       "      <td>อง</td>\n",
       "      <td>‌</td>\n",
       "      <td>有</td>\n",
       "      <td>و</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1   2   3   4   5   6   7   8   9    ... 554 555 556 557 558 559  \\\n",
       "Chars  ed  た。   m  ی   nd   d   と  د   ار   和  ...   ใ   ไ   h   d   ่   क   \n",
       "\n",
       "      560 561 562 563  \n",
       "Chars  อง   ‌   有   و  \n",
       "\n",
       "[1 rows x 564 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9c02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2d7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
