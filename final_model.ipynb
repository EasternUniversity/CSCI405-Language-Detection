{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf79ad63",
   "metadata": {},
   "source": [
    "# After all this work... Here is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26826488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports:\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd32752",
   "metadata": {},
   "source": [
    "# Reading in Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a6bd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdf9c8",
   "metadata": {},
   "source": [
    "# Splitting the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023d5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['Text'] # Feature matrix\n",
    "y = data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we are not modifying\n",
    "languages = set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b6ccd",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46d620",
   "metadata": {},
   "source": [
    "Creating a feature engineering function to turn raw text into data to be trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84aa1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function converts raw to prepped text (ready for training)\n",
    "def final_feature_eng(dataframe, chars, chars_2):\n",
    "    \n",
    "    # construct arrays\n",
    "    arr = dataframe.to_numpy() \n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    \n",
    "    # initialization of subscripts\n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    # iteratation pertaining to subscripts\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    \n",
    "    # construct additional arrays\n",
    "    arr_2 = dataframe.to_numpy()\n",
    "    new_arr_2 = np.zeros((len(arr_2), len(chars_2)))\n",
    "    \n",
    "    # initialization of subscripts\n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    # iteratation pertaining to subscripts\n",
    "    for text in arr_2:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars_2:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr_2[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "        \n",
    "    # list of specific languages\n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    \n",
    "    # output finalized DF after being feature engineered\n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d57a4",
   "metadata": {},
   "source": [
    "A function to feature engineer a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51e591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that modifies raw data into more prepped string data\n",
    "def test_function_2(dataframe, chars, chars_2):\n",
    "    \n",
    "    # construct array\n",
    "    new_arr = np.zeros((1, len(chars)))\n",
    "    \n",
    "    # inittialize subscript\n",
    "    j=0\n",
    "    \n",
    "    # interation pertaining to subscript\n",
    "    for char in chars:\n",
    "        count = 0.0\n",
    "        for letter in dataframe:\n",
    "            if letter == char:\n",
    "                count = count + 1.0\n",
    "            fraction = count/len(dataframe)\n",
    "        new_arr[0,j] = fraction\n",
    "        j = j+1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    \n",
    "    # construct another array\n",
    "    new_arr_2 = np.zeros((1, len(chars_2)))\n",
    "    \n",
    "    # initialize a counter and subcript\n",
    "    count = 0.0\n",
    "    j = 0\n",
    "    \n",
    "    # iteration pertaining to subcript and increments counter\n",
    "    for list in chars_2:\n",
    "        count = 0.0\n",
    "        for char in list:\n",
    "            for letter in dataframe:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "        fraction = count/len(dataframe)\n",
    "        new_arr_2[0,j] = fraction\n",
    "        j = j+1\n",
    "    \n",
    "    # list of specific languages\n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    \n",
    "    # output finalized DF after being feature engineered\n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a575d",
   "metadata": {},
   "source": [
    "# Final list of chars we are searching for in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b9bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_char_list_1 = ['î', 'û', 'ë', 'ï','ñ', 'â', 'î', 'ș', 'ş', 'ț', 'ç', 'ğ','ü', 'õ','å', 'ä', 'ö', 'a', 'b', \n",
    "                   'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "                   'v', 'w', 'x', 'y', 'z', 'ç', 'á', 'é', 'í', 'ó', 'ú', 'â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì',\n",
    "                   'ò', 'ù'] \n",
    "final_char_list_2 = [['ก', 'ข', 'ค', 'ฅ', 'ฆ','ง', 'จ', 'ฉ','ช', 'ฌ', 'ญ', 'ฎ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "                    'ธ', 'น', 'บ', 'บ', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ',  'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'เ', 'ปั', 'แ'],\n",
    "                    ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'а', 'е', 'ё', 'и', 'о', 'у', 'ы', 'э', 'ю', 'я', 'й'],\n",
    "                     ['ᄁ','ᄂ','ᄃ','ᄄ','ᄅᄆᄇ','ᄈ','ᄉ','ᄊ','ᄋ','ᄌᄍ','ᄎ','ᄏ','ᄐ','ᄑᄒ','아','악','안','알','암','압','앙','앞','애','액','앵야','얀','약','양','얘','어','억',\n",
    "                '언','얼','엄','업','엉','에','여','역','연','열','염','엽','영','예','ᄀ','여','역','연','열','염','엽','영','예','오','옥','온','올','옴','옹','와','완','왈','왕','왜','외','왼',\n",
    "                '요','욕','용','우','욱','운','울','움','웅','워','원','월','위','유','육','윤','율','융','윷','으','은','을','음','읍','응','의','이','익','인','일','임','입','잉','잎'],\n",
    "                    ['ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず',\n",
    "                  'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ',\n",
    "                  'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'れ', 'る', 'り', 'ら', 'よ', 'ょ', 'ゆ', 'ゅ', 'や', 'ゃ', 'も', 'め', 'む', 'み', 'ま', 'ぽ', 'ぼ', 'ほ', 'ぺ', 'ろ', 'ゎ',\n",
    "                  'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ゔ', 'ゕ', 'ゖ', ' ゚', '゛', '゜', 'ゝ','ゞ', 'ゟ', '゠', 'ァ', 'ア', 'サ', 'ゴ', 'コ', 'ゲ', 'ケ', 'グ', 'ク', 'ギ', 'キ',\n",
    "                  'ガ', 'カ', 'オ', 'ォ', 'エ', 'ェ', 'ウ', 'ゥ','イ', 'ィ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ' ,'ダ' ,'チ' ,'ヂ', 'ッ', 'ツ', 'ヅ',\n",
    "                  'テ', 'デ', 'ト', 'ホ', 'ペ', 'ベ', 'ヘ', 'プ', 'ブ', 'フ', 'ピ', 'ビ', 'ヒ', 'パ', 'バ', 'ハ', 'ノ', 'ネ', 'ヌ', 'ニ', 'ナ', 'ド', 'ボ', 'ポ', 'マ', 'ミ', \n",
    "                  'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', '㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ', 'ヷ',\n",
    "                  'ヶ', 'ヵ', 'ヴ', 'ン', 'ヲ', 'ヱ', 'ヰ', 'ワ'],\n",
    "                    ['胡', '赛', '尼', '本', '人', '和', '小', '说', '的', '主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是', '出', '生', '在', '阿', '富', '汗', '首', '都', \n",
    "                 '喀', '布', '尔', '少', '年', '时', '代', '便', '离', '开', '了', '这', '个', '国', '家', '。', '胡', '赛', '尼', '直', '到', '年', '小', '说', '出', '版', '之',\n",
    "                 '后', '才', '首', '次', '回', '到', '已', '经', '离', '开', '年', '的', '祖', '国', '他', '在', '苏', '联', '入', '侵', '时', '离', '开', '了', '阿', '富', \n",
    "                 '汗', '而', '他', '的', '很', '多', '童', '年', '好', '友', '在', '阿', '富', '汗', '生', '活', '在', '他', '们', '出', '发', '之', '前', '罗', '伯', '特', '伊',\n",
    "                 '达', '尔', '文', '卷', '查', '尔', '斯', '赖', '尔', '所', '著', '地', '质', '学', '原', '理', '在', '南', '美', '他', '得', '到', '第', '卷', '该', '书', '将'\n",
    "                 '地', '形', '地', '貌', '解', '释', '为', '漫', '长', '历', '史', '时', '间', '渐', '进', '演', '变', '的', '的', '结', '果', '当', '他', '旅', '程', '的', '第',\n",
    "                '站', '抵', '达', '圣', '地', '亚', '哥', '佛', '得', '角', '的', '时', '候', '达', '尔', '文'],\n",
    "                     ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'க', 'ங', 'ச', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ'],\n",
    "                     ['ش','س','ز','ر','ذ','د','خ','ح','ج','ث','ت','ب','ا','ء','ي','و','ه','ن','م','ل','ك','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ش','س','ژ','ز','ر','ذ','د','خ','ح','چ','ج','ث','ت','پ','ب','آ','ا','ص','ض','ط','ظ','ع','غ','ف','ق','ک','گ','ل','م','ن','و','ه','ی'],\n",
    "                     ['ش','س','ژ','ز','ڑ','ر','ذ','ڈ','د','خ','ح','چ',\n",
    "              'ج','ث','ٹ','ت','پ','ب','آ','ا','ے','ی','ھ','ہ','و','ں','ن','م','ل','گ','ک','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'प', 'ऩ', 'न', 'ध', 'द',\n",
    "               'थ', 'त', 'ण', 'ढ', 'ड', 'ठ', 'ट', 'ञ', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', '४', '३', '२', '१', '०', '॥', '।', 'ॡ', 'ॠ', 'ॐ', 'ऽ',\n",
    "               'ह', 'स', '५', '६', '७', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॾ', 'ॿ', 'ೱऀँं',' ः',' ऺ',' ऻ',' ा ',' ि',' ी',' ॎ',' ॏ',' ॗ'],\n",
    "                     ['ږ', 'ژ', 'ﺯ', 'ړ', 'ﺭ', 'ﺫ', 'ډ', 'ﺩ', 'ځ', 'څ', 'ﺥ', 'ځ', 'څ', 'ﺥ', 'ﺡ', 'چ', 'ﺝ', 'ﺙ', 'ټ', 'ﺕ', 'پ', 'ﺏ'\n",
    "                'ﻥ', 'ﻡ', 'ﻝ', 'ګ', 'ک', 'ﻕ', 'ﻑ', 'ﻍ', 'ﻉ', 'ﻅ', 'ﻁ', 'ﺽ', 'ﺹ', 'ښ', 'ﺵ', 'ۍ', 'ی', 'ې', 'ي', 'ۀ', 'ه', 'ﻭ']\n",
    "\n",
    "\n",
    "                    \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536417f",
   "metadata": {},
   "source": [
    "Using our feature engineering functions to convert raw data into trainable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93d7bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the feature engineering functions to the raw data\n",
    "\n",
    "# feature engineered training set\n",
    "X_train_ready = final_feature_eng(X_train.head(9000), final_char_list_1, final_char_list_2)\n",
    "\n",
    "# feature engineered testing set\n",
    "X_test_ready = final_feature_eng(X_test, final_char_list_1, final_char_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51723f5",
   "metadata": {},
   "source": [
    "Let's take a look at what the data that the Random Forest will train on looks like!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c28d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>î</th>\n",
       "      <th>û</th>\n",
       "      <th>ë</th>\n",
       "      <th>ï</th>\n",
       "      <th>ñ</th>\n",
       "      <th>â</th>\n",
       "      <th>î</th>\n",
       "      <th>ș</th>\n",
       "      <th>ş</th>\n",
       "      <th>ț</th>\n",
       "      <th>...</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Persian</th>\n",
       "      <th>Urdu</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Pushto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.480969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        î    û    ë    ï    ñ    â    î    ș    ş    ț  ...   Russian  Korean  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.853293     0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...     ...   \n",
       "8995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.835484     0.0   \n",
       "8996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "\n",
       "      Japanese  Chinese     Tamil  Arabic  Persian  Urdu  Hindi  Pushto  \n",
       "0     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "1     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "2     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "3     0.020761     0.00  0.480969     0.0      0.0   0.0    0.0     0.0  \n",
       "4     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "...        ...      ...       ...     ...      ...   ...    ...     ...  \n",
       "8995  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8996  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8997  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8998  0.660000     0.14  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8999  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "\n",
       "[9000 rows x 70 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data\n",
    "X_train_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336270",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier \n",
    "with grid searched hyperparameters... \n",
    "n_estimators=716,      min_samples_split=7,       max_depth=45,       max_leaf_nodes=50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff35c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate Random Forest Clf Model\n",
    "rnd_clf = RandomForestClassifier(n_estimators=716, min_samples_split=7, max_depth=45, max_leaf_nodes=50, random_state=42)\n",
    "\n",
    "# training our model\n",
    "rnd_clf.fit(X_train_ready, y_train.head(9000))\n",
    "\n",
    "# save our predictions\n",
    "y_preds = rnd_clf.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db771e2e",
   "metadata": {},
   "source": [
    "Accuracy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c245e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9536363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate accuracy score metric\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "# showcase our accuracy score\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e24dd",
   "metadata": {},
   "source": [
    "F1_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f274b54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9543637298316474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Combine Precision and Recall Scores with F1 Score\n",
    "# Instantiate F1 score metric\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "\n",
    "# showcase our F1 score \n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ad820",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using Rnd Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ac2de",
   "metadata": {},
   "source": [
    "just copy and paste whatever language you want to detect into the quotation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b3a68a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest model predicted the string to be the language of: ['Urdu']\n"
     ]
    }
   ],
   "source": [
    "# Random Unseen text instance\n",
    "string = \"اس نے اسے حفاظت سے پکڑ لیا کہ جگہ سے سیریڈ سٹیک کی بو آ رہی تھی۔ پتھر سڑک کے کنارے لگے ہوئے تھے کہ آگے کیا ہو سکتا ہےاسے یہ عجیب لگا کہ لوگ ایک دوسرے سے بات کرنے کے لیے اپنے سیل فون کا استعمال\"\n",
    "\n",
    "# feature engineer pertaining to new instance\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "\n",
    "# use our trained Random Forest model to predict the label of the new instance\n",
    "prediction = rnd_clf.predict(data_ready)\n",
    "\n",
    "# showcase the classification\n",
    "print('The random forest model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af9724",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e1967a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Instantiate a gradient booster clf model\n",
    "gbrt = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# train our model\n",
    "gbrt.fit(X_train_ready, y_train.head(9000))\n",
    "\n",
    "# save our predictions\n",
    "y_preds = gbrt.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cebc67",
   "metadata": {},
   "source": [
    "Accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b88a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.96\n"
     ]
    }
   ],
   "source": [
    "# instantiate an accuracy score metric\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "# showcase the accuracy score\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da14e1c",
   "metadata": {},
   "source": [
    "F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdc490ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9605333086874182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Combine Precision and Recall Scores with F1 Score\n",
    "# Instantiate F1 score metric\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "\n",
    "# showcase our F1 score\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bd5da",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using the Grd Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8e8ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient boosting classifier model predicted the string to be the language of: ['Russian']\n"
     ]
    }
   ],
   "source": [
    "# random unseen text instance\n",
    "string = \"Его застало врасплох тот факт, что в помещении пахло жареным стейком.vВалуны выстроились вдоль дороги, предвещая, что может произойти дальше.vЕй показалось странным, что люди используют свои мобильные телефоны, чтобы на самом деле разговаривать друг с другом.\"\n",
    "\n",
    "# feature engineer pertaining to new instance\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "\n",
    "# use our trained Gradient Booster model to predict the label of the new instance\n",
    "prediction = gbrt.predict(data_ready)\n",
    "\n",
    "# showcase the classification\n",
    "print('The gradient boosting classifier model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92ba8d",
   "metadata": {},
   "source": [
    "## Save Model and Dictionary for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1b5100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#Choose which model to save\n",
    "model = gbrt\n",
    "\n",
    "#Pickle the Model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "\n",
    "#Now save the dictionaries\n",
    "\n",
    "#Choose our dictionaries\n",
    "dict_latin = final_char_list_1\n",
    "dict_other = final_char_list_2\n",
    "\n",
    "#Save the latin alphabet to a text file\n",
    "with open('dict_latin', 'w', encoding='utf-8') as file:\n",
    "    for letter in dict_latin:\n",
    "        file.write('{},'.format(letter))\n",
    "\n",
    "#Save the other alphabets to another text file\n",
    "with open('dict_other', 'w', encoding='utf-8') as file2:\n",
    "    for alpha in dict_other:\n",
    "        for letter in alpha:\n",
    "            file2.write('{},'.format(letter))\n",
    "        file2.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57931691",
   "metadata": {},
   "source": [
    "## Now to test the Deployment functions that will go into the Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97b0d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data=\"English\"):\n",
    "    \n",
    "    #Load in Alphabets\n",
    "    alpha_latin = open('dict_latin', 'r', encoding='utf-8')\n",
    "    chars = alpha_latin.read().split(',')\n",
    "    chars.pop()\n",
    "    \n",
    "    with open('dict_other', 'r', encoding='utf-8') as alpha_other:\n",
    "        tmp = alpha_other.read().split('\\n')\n",
    "        chars_2 = [char.split(',') for char in tmp]\n",
    "        chars_2.pop()\n",
    "    \n",
    "    text = data\n",
    "        \n",
    "    new_arr = np.zeros((1, len(chars)))\n",
    "    j=0\n",
    "    for char in chars:\n",
    "        count = 0\n",
    "        for letter in text:\n",
    "            if letter == char:\n",
    "                count = count + 1\n",
    "        fraction = count/len(text)\n",
    "        new_arr[0,j] = fraction\n",
    "        j = j + 1\n",
    "\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "\n",
    "    #now second part of searching for chars\n",
    "    new_arr_2 = np.zeros((1, len(chars_2)))\n",
    "    count = 0.0\n",
    "    j = 0\n",
    "    for list in chars_2:\n",
    "        count = 0.0\n",
    "        for char in list:\n",
    "            for letter in text:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "        fraction = count/len(text)\n",
    "        new_arr_2[0,j] = fraction\n",
    "        j = j+1\n",
    "    \n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    return final_data_frame\n",
    "\n",
    "def modelPredict(dataFrame):\n",
    "\n",
    "    data = dataFrame\n",
    "    \n",
    "    #Grab a Pickle (Open PKL File)\n",
    "    file = open('model.pkl', 'rb')\n",
    "\n",
    "    #Eat the pickle (Load the Pickled Model)\n",
    "    model = joblib.load(file)\n",
    "\n",
    "    #See if you ate the right pickle (Predict)\n",
    "    predicition = model.predict(data)\n",
    "\n",
    "    return predicition[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "620ae062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Spanish Russian Urdu\n"
     ]
    }
   ],
   "source": [
    "#Lets use some sampe text!\n",
    "text_en = \"The quick brown fox jumps over the lazy dog\"\n",
    "text_es = \"Es un área definida de la superficie, ya sea de tierra, agua o hielo propuesto para la llegada, salida y movimiento en superficie de aeronaves de distintos tipos con llegadas y salidas nacionales e internacionales.\"\n",
    "text_ru = \"Его застало врасплох тот факт, что в помещении пахло жареным стейком.vВалуны выстроились вдоль дороги, предвещая, что может произойти дальше.vЕй показалось странным, что люди используют свои мобильные телефоны, чтобы на самом деле разговаривать друг с другом.\"\n",
    "text_push = \"اس نے اسے حفاظت سے پکڑ لیا کہ جگہ سے سیریڈ سٹیک کی بو آ رہی تھی۔ پتھر سڑک کے کنارے لگے ہوئے تھے کہ آگے کیا ہو سکتا ہےاسے یہ عجیب لگا کہ لوگ ایک دوسرے سے بات کرنے کے لیے اپنے سیل فون کا استعمال\"\n",
    "\n",
    "#-----Now lets run them through the functions!-----#\n",
    "\n",
    "#Processing Data\n",
    "frame_en = processData(text_en)\n",
    "frame_es = processData(text_es)\n",
    "frame_ru = processData(text_ru)\n",
    "frame_push = processData(text_push)\n",
    "\n",
    "#Get the Prediction\n",
    "predict_en = modelPredict(frame_en)\n",
    "predict_es = modelPredict(frame_es)\n",
    "predict_ru = modelPredict(frame_ru)\n",
    "predict_push = modelPredict(frame_push)\n",
    "\n",
    "print(predict_en, predict_es, predict_ru, predict_push)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
