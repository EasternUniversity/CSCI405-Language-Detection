{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4ea32a",
   "metadata": {},
   "source": [
    "# After all this work... Here is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3090f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports:\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d339a7c",
   "metadata": {},
   "source": [
    "# Reading in Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236262bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e9a2e",
   "metadata": {},
   "source": [
    "# Splitting the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6790a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "languages = set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663568c4",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd698c03",
   "metadata": {},
   "source": [
    "Creating a feature engineering function to turn raw text into data to be trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e2f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_feature_eng(dataframe, chars, chars_2):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    #now second function thing\n",
    "    arr_2 = dataframe.to_numpy()\n",
    "    new_arr_2 = np.zeros((len(arr_2), len(chars_2)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr_2:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars_2:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr_2[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    \n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eccefb6",
   "metadata": {},
   "source": [
    "A function to feature engineer a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f07ab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function_2(dataframe, chars, chars_2):\n",
    "    new_arr = np.zeros((1, len(chars)))\n",
    "    j=0\n",
    "    for char in chars:\n",
    "        count = 0.0\n",
    "        for letter in dataframe:\n",
    "            if letter == char:\n",
    "                count = count + 1.0\n",
    "            fraction = count/len(dataframe)\n",
    "        new_arr[0,j] = fraction\n",
    "        j = j+1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    #now second part of searching for chars\n",
    "    new_arr_2 = np.zeros((1, len(chars_2)))\n",
    "    count = 0.0\n",
    "    j = 0\n",
    "    for list in chars_2:\n",
    "        count = 0.0\n",
    "        for char in list:\n",
    "            for letter in dataframe:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "        fraction = count/len(dataframe)\n",
    "        new_arr_2[0,j] = fraction\n",
    "        j = j+1\n",
    "    \n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc18f408",
   "metadata": {},
   "source": [
    "# Final list of chars we are searching for in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0867fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_char_list_1 = ['î', 'û', 'ë', 'ï','ñ', 'â', 'î', 'ș', 'ş', 'ț', 'ç', 'ğ','ü', 'õ','å', 'ä', 'ö', 'a', 'b', \n",
    "                   'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "                   'v', 'w', 'x', 'y', 'z', 'ç', 'á', 'é', 'í', 'ó', 'ú', 'â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì',\n",
    "                   'ò', 'ù'] \n",
    "final_char_list_2 = [['ก', 'ข', 'ค', 'ฅ', 'ฆ','ง', 'จ', 'ฉ','ช', 'ฌ', 'ญ', 'ฎ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "                    'ธ', 'น', 'บ', 'บ', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ',  'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'เ', 'ปั', 'แ'],\n",
    "                    ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'а', 'е', 'ё', 'и', 'о', 'у', 'ы', 'э', 'ю', 'я', 'й'],\n",
    "                     ['ᄁ','ᄂ','ᄃ','ᄄ','ᄅᄆᄇ','ᄈ','ᄉ','ᄊ','ᄋ','ᄌᄍ','ᄎ','ᄏ','ᄐ','ᄑᄒ','아','악','안','알','암','압','앙','앞','애','액','앵야','얀','약','양','얘','어','억',\n",
    "                '언','얼','엄','업','엉','에','여','역','연','열','염','엽','영','예','ᄀ','여','역','연','열','염','엽','영','예','오','옥','온','올','옴','옹','와','완','왈','왕','왜','외','왼',\n",
    "                '요','욕','용','우','욱','운','울','움','웅','워','원','월','위','유','육','윤','율','융','윷','으','은','을','음','읍','응','의','이','익','인','일','임','입','잉','잎'],\n",
    "                    ['ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず',\n",
    "                  'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ',\n",
    "                  'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'れ', 'る', 'り', 'ら', 'よ', 'ょ', 'ゆ', 'ゅ', 'や', 'ゃ', 'も', 'め', 'む', 'み', 'ま', 'ぽ', 'ぼ', 'ほ', 'ぺ', 'ろ', 'ゎ',\n",
    "                  'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ゔ', 'ゕ', 'ゖ', ' ゚', '゛', '゜', 'ゝ','ゞ', 'ゟ', '゠', 'ァ', 'ア', 'サ', 'ゴ', 'コ', 'ゲ', 'ケ', 'グ', 'ク', 'ギ', 'キ',\n",
    "                  'ガ', 'カ', 'オ', 'ォ', 'エ', 'ェ', 'ウ', 'ゥ','イ', 'ィ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ' ,'ダ' ,'チ' ,'ヂ', 'ッ', 'ツ', 'ヅ',\n",
    "                  'テ', 'デ', 'ト', 'ホ', 'ペ', 'ベ', 'ヘ', 'プ', 'ブ', 'フ', 'ピ', 'ビ', 'ヒ', 'パ', 'バ', 'ハ', 'ノ', 'ネ', 'ヌ', 'ニ', 'ナ', 'ド', 'ボ', 'ポ', 'マ', 'ミ', \n",
    "                  'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', '㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ', 'ヷ',\n",
    "                  'ヶ', 'ヵ', 'ヴ', 'ン', 'ヲ', 'ヱ', 'ヰ', 'ワ'],\n",
    "                    ['胡', '赛', '尼', '本', '人', '和', '小', '说', '的', '主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是', '出', '生', '在', '阿', '富', '汗', '首', '都', \n",
    "                 '喀', '布', '尔', '少', '年', '时', '代', '便', '离', '开', '了', '这', '个', '国', '家', '。', '胡', '赛', '尼', '直', '到', '年', '小', '说', '出', '版', '之',\n",
    "                 '后', '才', '首', '次', '回', '到', '已', '经', '离', '开', '年', '的', '祖', '国', '他', '在', '苏', '联', '入', '侵', '时', '离', '开', '了', '阿', '富', \n",
    "                 '汗', '而', '他', '的', '很', '多', '童', '年', '好', '友', '在', '阿', '富', '汗', '生', '活', '在', '他', '们', '出', '发', '之', '前', '罗', '伯', '特', '伊',\n",
    "                 '达', '尔', '文', '卷', '查', '尔', '斯', '赖', '尔', '所', '著', '地', '质', '学', '原', '理', '在', '南', '美', '他', '得', '到', '第', '卷', '该', '书', '将'\n",
    "                 '地', '形', '地', '貌', '解', '释', '为', '漫', '长', '历', '史', '时', '间', '渐', '进', '演', '变', '的', '的', '结', '果', '当', '他', '旅', '程', '的', '第',\n",
    "                '站', '抵', '达', '圣', '地', '亚', '哥', '佛', '得', '角', '的', '时', '候', '达', '尔', '文'],\n",
    "                     ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'க', 'ங', 'ச', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ'],\n",
    "                     ['ش','س','ز','ر','ذ','د','خ','ح','ج','ث','ت','ب','ا','ء','ي','و','ه','ن','م','ل','ك','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ش','س','ژ','ز','ر','ذ','د','خ','ح','چ','ج','ث','ت','پ','ب','آ','ا','ص','ض','ط','ظ','ع','غ','ف','ق','ک','گ','ل','م','ن','و','ه','ی'],\n",
    "                     ['ش','س','ژ','ز','ڑ','ر','ذ','ڈ','د','خ','ح','چ',\n",
    "              'ج','ث','ٹ','ت','پ','ب','آ','ا','ے','ی','ھ','ہ','و','ں','ن','م','ل','گ','ک','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'प', 'ऩ', 'न', 'ध', 'द',\n",
    "               'थ', 'त', 'ण', 'ढ', 'ड', 'ठ', 'ट', 'ञ', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', '४', '३', '२', '१', '०', '॥', '।', 'ॡ', 'ॠ', 'ॐ', 'ऽ',\n",
    "               'ह', 'स', '५', '६', '७', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॾ', 'ॿ', 'ೱऀँं',' ः',' ऺ',' ऻ',' ा ',' ि',' ी',' ॎ',' ॏ',' ॗ'],\n",
    "                     ['ږ', 'ژ', 'ﺯ', 'ړ', 'ﺭ', 'ﺫ', 'ډ', 'ﺩ', 'ځ', 'څ', 'ﺥ', 'ځ', 'څ', 'ﺥ', 'ﺡ', 'چ', 'ﺝ', 'ﺙ', 'ټ', 'ﺕ', 'پ', 'ﺏ'\n",
    "                'ﻥ', 'ﻡ', 'ﻝ', 'ګ', 'ک', 'ﻕ', 'ﻑ', 'ﻍ', 'ﻉ', 'ﻅ', 'ﻁ', 'ﺽ', 'ﺹ', 'ښ', 'ﺵ', 'ۍ', 'ی', 'ې', 'ي', 'ۀ', 'ه', 'ﻭ']\n",
    "\n",
    "\n",
    "                    \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc44ae6b",
   "metadata": {},
   "source": [
    "Using our feature engineering functions to convert raw data into trainable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dbb01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ready = final_feature_eng(X_train.head(9000), final_char_list_1, final_char_list_2)\n",
    "X_test_ready = final_feature_eng(X_test, final_char_list_1, final_char_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da589c9a",
   "metadata": {},
   "source": [
    "Let's take a look at what the data that the random forrest is training on looks like!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dada5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>î</th>\n",
       "      <th>û</th>\n",
       "      <th>ë</th>\n",
       "      <th>ï</th>\n",
       "      <th>ñ</th>\n",
       "      <th>â</th>\n",
       "      <th>î</th>\n",
       "      <th>ș</th>\n",
       "      <th>ş</th>\n",
       "      <th>ț</th>\n",
       "      <th>...</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Persian</th>\n",
       "      <th>Urdu</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Pushto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.480969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        î    û    ë    ï    ñ    â    î    ș    ş    ț  ...   Russian  Korean  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.853293     0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...     ...   \n",
       "8995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.835484     0.0   \n",
       "8996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "\n",
       "      Japanese  Chinese     Tamil  Arabic  Persian  Urdu  Hindi  Pushto  \n",
       "0     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "1     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "2     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "3     0.020761     0.00  0.480969     0.0      0.0   0.0    0.0     0.0  \n",
       "4     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "...        ...      ...       ...     ...      ...   ...    ...     ...  \n",
       "8995  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8996  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8997  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8998  0.660000     0.14  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8999  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "\n",
       "[9000 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a4101",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier \n",
    "with grid searched hyperparameters... \n",
    "n_estimators=716,      min_samples_split=7,       max_depth=45,       max_leaf_nodes=50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220d27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=716, min_samples_split=7, max_depth=45, max_leaf_nodes=50, random_state=42)\n",
    "rnd_clf.fit(X_train_ready, y_train.head(9000))\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52d2a6",
   "metadata": {},
   "source": [
    "Accuracy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98668df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9536363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a45096",
   "metadata": {},
   "source": [
    "F1_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5687c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9543637298316474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469bca7",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using Rnd Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f5901",
   "metadata": {},
   "source": [
    "just copy and paste whatever language you want to detect into the quotation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d25ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest model predicted the string to be the language of: ['Urdu']\n"
     ]
    }
   ],
   "source": [
    "string = \"اس نے اسے حفاظت سے پکڑ لیا کہ جگہ سے سیریڈ سٹیک کی بو آ رہی تھی۔ پتھر سڑک کے کنارے لگے ہوئے تھے کہ آگے کیا ہو سکتا ہےاسے یہ عجیب لگا کہ لوگ ایک دوسرے سے بات کرنے کے لیے اپنے سیل فون کا استعمال\"\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "prediction = rnd_clf.predict(data_ready)\n",
    "print('The random forest model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b9851",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ffb34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=42)\n",
    "gbrt.fit(X_train_ready, y_train.head(9000))\n",
    "y_preds = gbrt.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c4ab1",
   "metadata": {},
   "source": [
    "Accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d452b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9605333086874182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782ef17",
   "metadata": {},
   "source": [
    "F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da192155",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/wh0vr5s52d91hb2v0b3n0tm40000gn/T/ipykernel_33529/3173792149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1_score=%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c592d8",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using the Grd Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f71c3fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient boosting classifier model predicted the string to be the language of: ['Urdu']\n"
     ]
    }
   ],
   "source": [
    "string = \"اس نے اسے حفاظت سے پکڑ لیا کہ جگہ سے سیریڈ سٹیک کی بو آ رہی تھی۔ پتھر سڑک کے کنارے لگے ہوئے تھے کہ آگے کیا ہو سکتا ہےاسے یہ عجیب لگا کہ لوگ ایک دوسرے سے بات کرنے کے لیے اپنے سیل فون کا استعمال\"\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "prediction = gbrt.predict(data_ready)\n",
    "print('The gradient boosting classifier model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0897152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
