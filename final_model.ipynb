{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cdbba45",
   "metadata": {},
   "source": [
    "# After all this work... Here is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72c9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports:\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e6434",
   "metadata": {},
   "source": [
    "# Reading in Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2158ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201488d1",
   "metadata": {},
   "source": [
    "# Splitting the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40bbcc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "languages = set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bb1b9",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ddad9",
   "metadata": {},
   "source": [
    "Creating a feature engineering function to turn raw text into data to be trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73058223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_feature_eng(dataframe, chars, chars_2):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    #now second function thing\n",
    "    arr_2 = dataframe.to_numpy()\n",
    "    new_arr_2 = np.zeros((len(arr_2), len(chars_2)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr_2:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars_2:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr_2[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    \n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad776a",
   "metadata": {},
   "source": [
    "A function to feature engineer a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abba0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function_2(dataframe, chars, chars_2):\n",
    "    new_arr = np.zeros((1, len(chars)))\n",
    "    j=0\n",
    "    for char in chars:\n",
    "        count = 0.0\n",
    "        for letter in dataframe:\n",
    "            if letter == char:\n",
    "                count = count + 1.0\n",
    "            fraction = count/len(dataframe)\n",
    "        new_arr[0,j] = fraction\n",
    "        j = j+1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    #now second part of searching for chars\n",
    "    new_arr_2 = np.zeros((1, len(chars_2)))\n",
    "    count = 0.0\n",
    "    j = 0\n",
    "    for list in chars_2:\n",
    "        count = 0.0\n",
    "        for char in list:\n",
    "            for letter in dataframe:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "        fraction = count/len(dataframe)\n",
    "        new_arr_2[0,j] = fraction\n",
    "        j = j+1\n",
    "    \n",
    "    names = ['Thai', 'Russian', 'Korean', 'Japanese', 'Chinese', 'Tamil', 'Arabic', 'Persian', 'Urdu', 'Hindi', 'Pushto']\n",
    "    \n",
    "    data_frame_2 = pd.DataFrame(new_arr_2, columns = names)\n",
    "    \n",
    "    final_data_frame = pd.concat([data_frame, data_frame_2], axis=1)\n",
    "    return final_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f542f99",
   "metadata": {},
   "source": [
    "# Final list of chars we are searching for in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0e4214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_char_list_1 = ['î', 'û', 'ë', 'ï','ñ', 'â', 'î', 'ș', 'ş', 'ț', 'ç', 'ğ','ü', 'õ','å', 'ä', 'ö', 'a', 'b', \n",
    "                   'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', \n",
    "                   'v', 'w', 'x', 'y', 'z', 'ç', 'á', 'é', 'í', 'ó', 'ú', 'â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì',\n",
    "                   'ò', 'ù'] \n",
    "final_char_list_2 = [['ก', 'ข', 'ค', 'ฅ', 'ฆ','ง', 'จ', 'ฉ','ช', 'ฌ', 'ญ', 'ฎ', 'ฐ', 'ฑ', 'ฒ', 'ณ', 'ด', 'ต', 'ถ', 'ท',\n",
    "                    'ธ', 'น', 'บ', 'บ', 'ผ', 'ฝ', 'พ', 'ฟ', 'ภ',  'ม', 'ย', 'ร', 'ล', 'ว', 'ศ', 'ษ', 'ส', 'ห', 'ฬ', 'อ', 'ฮ', 'เ', 'ปั', 'แ'],\n",
    "                    ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н', 'п', 'р', 'с', 'т', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'а', 'е', 'ё', 'и', 'о', 'у', 'ы', 'э', 'ю', 'я', 'й'],\n",
    "                     ['ᄁ','ᄂ','ᄃ','ᄄ','ᄅᄆᄇ','ᄈ','ᄉ','ᄊ','ᄋ','ᄌᄍ','ᄎ','ᄏ','ᄐ','ᄑᄒ','아','악','안','알','암','압','앙','앞','애','액','앵야','얀','약','양','얘','어','억',\n",
    "                '언','얼','엄','업','엉','에','여','역','연','열','염','엽','영','예','ᄀ','여','역','연','열','염','엽','영','예','오','옥','온','올','옴','옹','와','완','왈','왕','왜','외','왼',\n",
    "                '요','욕','용','우','욱','운','울','움','웅','워','원','월','위','유','육','윤','율','융','윷','으','은','을','음','읍','응','의','이','익','인','일','임','입','잉','잎'],\n",
    "                    ['ぁ', 'あ', 'ぃ', 'い', 'ぅ', 'う', 'ぇ', 'え', 'ぉ', 'お', 'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご', 'さ', 'ざ', 'し', 'じ', 'す', 'ず',\n",
    "                  'せ', 'ぜ', 'そ', 'ぞ', 'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど', 'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ',\n",
    "                  'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'れ', 'る', 'り', 'ら', 'よ', 'ょ', 'ゆ', 'ゅ', 'や', 'ゃ', 'も', 'め', 'む', 'み', 'ま', 'ぽ', 'ぼ', 'ほ', 'ぺ', 'ろ', 'ゎ',\n",
    "                  'わ', 'ゐ', 'ゑ', 'を', 'ん', 'ゔ', 'ゕ', 'ゖ', ' ゚', '゛', '゜', 'ゝ','ゞ', 'ゟ', '゠', 'ァ', 'ア', 'サ', 'ゴ', 'コ', 'ゲ', 'ケ', 'グ', 'ク', 'ギ', 'キ',\n",
    "                  'ガ', 'カ', 'オ', 'ォ', 'エ', 'ェ', 'ウ', 'ゥ','イ', 'ィ', 'ザ', 'シ', 'ジ', 'ス', 'ズ', 'セ', 'ゼ', 'ソ', 'ゾ', 'タ' ,'ダ' ,'チ' ,'ヂ', 'ッ', 'ツ', 'ヅ',\n",
    "                  'テ', 'デ', 'ト', 'ホ', 'ペ', 'ベ', 'ヘ', 'プ', 'ブ', 'フ', 'ピ', 'ビ', 'ヒ', 'パ', 'バ', 'ハ', 'ノ', 'ネ', 'ヌ', 'ニ', 'ナ', 'ド', 'ボ', 'ポ', 'マ', 'ミ', \n",
    "                  'ム', 'メ', 'モ', 'ャ', 'ヤ', 'ュ', 'ユ', 'ョ', 'ヨ', 'ラ', 'リ', 'ル', 'レ', 'ロ', 'ヮ', '㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ', 'ヷ',\n",
    "                  'ヶ', 'ヵ', 'ヴ', 'ン', 'ヲ', 'ヱ', 'ヰ', 'ワ'],\n",
    "                    ['胡', '赛', '尼', '本', '人', '和', '小', '说', '的', '主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是', '出', '生', '在', '阿', '富', '汗', '首', '都', \n",
    "                 '喀', '布', '尔', '少', '年', '时', '代', '便', '离', '开', '了', '这', '个', '国', '家', '。', '胡', '赛', '尼', '直', '到', '年', '小', '说', '出', '版', '之',\n",
    "                 '后', '才', '首', '次', '回', '到', '已', '经', '离', '开', '年', '的', '祖', '国', '他', '在', '苏', '联', '入', '侵', '时', '离', '开', '了', '阿', '富', \n",
    "                 '汗', '而', '他', '的', '很', '多', '童', '年', '好', '友', '在', '阿', '富', '汗', '生', '活', '在', '他', '们', '出', '发', '之', '前', '罗', '伯', '特', '伊',\n",
    "                 '达', '尔', '文', '卷', '查', '尔', '斯', '赖', '尔', '所', '著', '地', '质', '学', '原', '理', '在', '南', '美', '他', '得', '到', '第', '卷', '该', '书', '将'\n",
    "                 '地', '形', '地', '貌', '解', '释', '为', '漫', '长', '历', '史', '时', '间', '渐', '进', '演', '变', '的', '的', '结', '果', '当', '他', '旅', '程', '的', '第',\n",
    "                '站', '抵', '达', '圣', '地', '亚', '哥', '佛', '得', '角', '的', '时', '候', '达', '尔', '文'],\n",
    "                     ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'க', 'ங', 'ச', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ'],\n",
    "                     ['ش','س','ز','ر','ذ','د','خ','ح','ج','ث','ت','ب','ا','ء','ي','و','ه','ن','م','ل','ك','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ش','س','ژ','ز','ر','ذ','د','خ','ح','چ','ج','ث','ت','پ','ب','آ','ا','ص','ض','ط','ظ','ع','غ','ف','ق','ک','گ','ل','م','ن','و','ه','ی'],\n",
    "                     ['ش','س','ژ','ز','ڑ','ر','ذ','ڈ','د','خ','ح','چ',\n",
    "              'ج','ث','ٹ','ت','پ','ب','آ','ا','ے','ی','ھ','ہ','و','ں','ن','م','ل','گ','ک','ق','ف','غ','ع','ظ','ط','ض','ص'],\n",
    "                     ['ऄ', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ऍ', 'ऎ', 'ए', 'ऐ', 'ऑ', 'ऒ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'प', 'ऩ', 'न', 'ध', 'द',\n",
    "               'थ', 'त', 'ण', 'ढ', 'ड', 'ठ', 'ट', 'ञ', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ऱ', 'ल', 'ळ', 'ऴ', 'व', 'श', 'ष', '४', '३', '२', '१', '०', '॥', '।', 'ॡ', 'ॠ', 'ॐ', 'ऽ',\n",
    "               'ह', 'स', '५', '६', '७', 'ॲ', 'ॳ', 'ॴ', 'ॵ', 'ॶ', 'ॷ', 'ॹ', 'ॺ', 'ॻ', 'ॼ', 'ॾ', 'ॿ', 'ೱऀँं',' ः',' ऺ',' ऻ',' ा ',' ि',' ी',' ॎ',' ॏ',' ॗ'],\n",
    "                     ['ږ', 'ژ', 'ﺯ', 'ړ', 'ﺭ', 'ﺫ', 'ډ', 'ﺩ', 'ځ', 'څ', 'ﺥ', 'ځ', 'څ', 'ﺥ', 'ﺡ', 'چ', 'ﺝ', 'ﺙ', 'ټ', 'ﺕ', 'پ', 'ﺏ'\n",
    "                'ﻥ', 'ﻡ', 'ﻝ', 'ګ', 'ک', 'ﻕ', 'ﻑ', 'ﻍ', 'ﻉ', 'ﻅ', 'ﻁ', 'ﺽ', 'ﺹ', 'ښ', 'ﺵ', 'ۍ', 'ی', 'ې', 'ي', 'ۀ', 'ه', 'ﻭ']\n",
    "\n",
    "\n",
    "                    \n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659760a",
   "metadata": {},
   "source": [
    "Using our feature engineering functions to convert raw data into trainable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e25e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ready = final_feature_eng(X_train.head(9000), final_char_list_1, final_char_list_2)\n",
    "X_test_ready = final_feature_eng(X_test, final_char_list_1, final_char_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac99dd",
   "metadata": {},
   "source": [
    "Let's take a look at what the data that the random forrest is training on looks like!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5b3d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>î</th>\n",
       "      <th>û</th>\n",
       "      <th>ë</th>\n",
       "      <th>ï</th>\n",
       "      <th>ñ</th>\n",
       "      <th>â</th>\n",
       "      <th>î</th>\n",
       "      <th>ș</th>\n",
       "      <th>ş</th>\n",
       "      <th>ț</th>\n",
       "      <th>...</th>\n",
       "      <th>Russian</th>\n",
       "      <th>Korean</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>Tamil</th>\n",
       "      <th>Arabic</th>\n",
       "      <th>Persian</th>\n",
       "      <th>Urdu</th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Pushto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.480969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        î    û    ë    ï    ñ    â    î    ș    ş    ț  ...   Russian  Korean  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.853293     0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...     ...   \n",
       "8995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.835484     0.0   \n",
       "8996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "8999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000000     0.0   \n",
       "\n",
       "      Japanese  Chinese     Tamil  Arabic  Persian  Urdu  Hindi  Pushto  \n",
       "0     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "1     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "2     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "3     0.020761     0.00  0.480969     0.0      0.0   0.0    0.0     0.0  \n",
       "4     0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "...        ...      ...       ...     ...      ...   ...    ...     ...  \n",
       "8995  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8996  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8997  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8998  0.660000     0.14  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "8999  0.000000     0.00  0.000000     0.0      0.0   0.0    0.0     0.0  \n",
       "\n",
       "[9000 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e530d9",
   "metadata": {},
   "source": [
    "# Random Forrest Classifier \n",
    "with grid searched hyperparameters... \n",
    "n_estimators=716,      min_samples_split=7,       max_depth=45,       max_leaf_nodes=50,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6dc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=716, min_samples_split=7, max_depth=45, max_leaf_nodes=50, random_state=42)\n",
    "rnd_clf.fit(X_train_ready, y_train.head(9000))\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba746d19",
   "metadata": {},
   "source": [
    "Accuracy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4132e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9536363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216d0566",
   "metadata": {},
   "source": [
    "F1_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56dd0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9543637298316474\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835c470",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using Rnd Forest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc210206",
   "metadata": {},
   "source": [
    "just copy and paste whatever language you want to detect into the quotation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b472a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random forest model predicted the string to be the language of: ['Urdu']\n"
     ]
    }
   ],
   "source": [
    "string = \"اس نے اسے حفاظت سے پکڑ لیا کہ جگہ سے سیریڈ سٹیک کی بو آ رہی تھی۔ پتھر سڑک کے کنارے لگے ہوئے تھے کہ آگے کیا ہو سکتا ہےاسے یہ عجیب لگا کہ لوگ ایک دوسرے سے بات کرنے کے لیے اپنے سیل فون کا استعمال\"\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "prediction = rnd_clf.predict(data_ready)\n",
    "print('The random forest model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50acf1",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82c64e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=42)\n",
    "gbrt.fit(X_train_ready, y_train.head(9000))\n",
    "y_preds = gbrt.predict(X_test_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83801dd5",
   "metadata": {},
   "source": [
    "Accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "686d1448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.96\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477c6be",
   "metadata": {},
   "source": [
    "F1 Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "905bfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score=0.9605333086874182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score = f1_score(y_test, y_preds, average='weighted')\n",
    "print('F1_score=%s' % (f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23349f1",
   "metadata": {},
   "source": [
    "# Predicting the language of a string using the Grd Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b63d9daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient boosting classifier model predicted the string to be the language of: ['Russian']\n"
     ]
    }
   ],
   "source": [
    "string = \"Его застало врасплох тот факт, что в помещении пахло жареным стейком.vВалуны выстроились вдоль дороги, предвещая, что может произойти дальше.vЕй показалось странным, что люди используют свои мобильные телефоны, чтобы на самом деле разговаривать друг с другом.\"\n",
    "data_ready = test_function_2(string, final_char_list_1, final_char_list_2)\n",
    "prediction = gbrt.predict(data_ready)\n",
    "print('The gradient boosting classifier model predicted the string to be the language of: %s' % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f352e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
