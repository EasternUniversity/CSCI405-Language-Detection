{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd55be",
   "metadata": {},
   "source": [
    "# Understanding the Kaggle Data\n",
    "The dataset acquired from [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/data) contains words from several different languages. The noise contained in the dataset are duplicate words. To reduce this noise, the words will be broken down into single and double characters, then rated based on how often they show up in that respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data*\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18753df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "languages = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)      \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30accace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_2(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['english', 'estonian', 'swedish', 'thai', 'tamil', 'dutch', 'japanese', 'turkish', 'latin', 'urdu',\n",
    "             'indonesian', 'portuguese', 'french', 'chinese', 'korean', 'hindi', 'spanish', 'pushto', 'persian',\n",
    "             'romanian', 'russian', 'arabic']\n",
    "    \n",
    "    data_frame = pd.DataFrame(new_arr, columns = names)\n",
    "    return data_frame\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41028870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>estonian</th>\n",
       "      <th>swedish</th>\n",
       "      <th>thai</th>\n",
       "      <th>tamil</th>\n",
       "      <th>dutch</th>\n",
       "      <th>japanese</th>\n",
       "      <th>turkish</th>\n",
       "      <th>latin</th>\n",
       "      <th>urdu</th>\n",
       "      <th>...</th>\n",
       "      <th>french</th>\n",
       "      <th>chinese</th>\n",
       "      <th>korean</th>\n",
       "      <th>hindi</th>\n",
       "      <th>spanish</th>\n",
       "      <th>pushto</th>\n",
       "      <th>persian</th>\n",
       "      <th>romanian</th>\n",
       "      <th>russian</th>\n",
       "      <th>arabic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192913</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english  estonian   swedish      thai     tamil     dutch  japanese  \\\n",
       "0  0.000000  0.000000  0.000000  0.118919  0.000000  0.000000       0.0   \n",
       "1  0.000000  0.000000  0.000000  0.157277  0.000000  0.000000       0.0   \n",
       "2  0.523622  0.263780  0.523622  0.000000  0.000000  0.523622       0.0   \n",
       "3  0.038062  0.013841  0.031142  0.000000  0.034602  0.038062       0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000       0.0   \n",
       "\n",
       "    turkish     latin  urdu  ...  french  chinese  korean  hindi   spanish  \\\n",
       "0  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "1  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "2  0.192913  0.598425   0.0  ...     0.0      0.0     0.0    0.0  0.208661   \n",
       "3  0.006920  0.031142   0.0  ...     0.0      0.0     0.0    0.0  0.013841   \n",
       "4  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "\n",
       "   pushto  persian  romanian   russian  arabic  \n",
       "0     0.0      0.0       0.0  0.000000     0.0  \n",
       "1     0.0      0.0       0.0  0.000000     0.0  \n",
       "2     0.0      0.0       0.0  0.000000     0.0  \n",
       "3     0.0      0.0       0.0  0.000000     0.0  \n",
       "4     0.0      0.0       0.0  0.264471     0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]\n",
    "\n",
    "panda = X_train.head()\n",
    "some_data = feature_engineering_2(panda, new_chars)\n",
    "some_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2238993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8252272727272727\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = X_train.head(6000)\n",
    "X_train_2_eng = feature_engineering_2(X_train_2, new_chars)\n",
    "\n",
    "y_train_2 = y_train.head(6000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf_2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf_2.fit(X_train_2_eng, y_train_2)\n",
    "\n",
    "X_test_2 = feature_engineering_2(X_test, new_chars)\n",
    "y_preds_2 = rnd_clf_2.predict(X_test_2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score_2 = accuracy_score(y_test, y_preds_2)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1520ba8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8822727272727273\n"
     ]
    }
   ],
   "source": [
    "#First Model Trained\n",
    "#characters for first model\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "X_train_2 = X_train.head(3000)\n",
    "model_one = feature_engineering(X_train_2, chars_2)\n",
    "model_one\n",
    "\n",
    "y_train_one = y_train.head(3000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(model_one, y_train_one)\n",
    "\n",
    "X_test_1 = feature_engineering(X_test, chars_2)\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))\n",
    "\n",
    "import pickle #ask about pickle\n",
    "\n",
    "saved_model = pickle.dumps(rnd_clf)\n",
    "\n",
    "rdf_from_pickle = pickle.loads(saved_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328473b2",
   "metadata": {},
   "source": [
    "# Creating Features: Most Used Characters\n",
    "\n",
    "To train the models to determine which language is being used by the user, we first need to know which characters are used in each language. The best approach for this is to used the dataset, which is already using the characters from each language, and find the most used characters in them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db2a28",
   "metadata": {},
   "source": [
    "# Kaggle Resources\n",
    "Kaggle Found that there is a performance rate of 97% of Correctly matching characters using a mixture of the top 50 single and double characters used in their words dataset. This is code extracted from their findings\n",
    "\n",
    "The following code is taken from the Kaggle to understand how to process characters for each language. Kaggle uses the CountVectorizor Method from the sklearn module to tokenize the characters into readable 1's and 0's. Then, it counts how many times that character has been used in the sample data provided.\n",
    "\n",
    "This method reduces the necessity of locating alphabets for each language and creating custom functions to find the most used characters in the dataset.\n",
    "\n",
    "To summarize Kaggle's findings: Languages based off the Latin Alphabet are easier to differentiate from each other in the data set while Languages with their own Alphabet, like Chinese and Japanese, can be differentiated by single characters alone.\n",
    "\n",
    "This code here fits and transforms the X_train and X_test data sets into readable 1's and 0's and counts the number of times a specific character shows up in the datasets.The min_df parameter tells CountVectorizor to save any characters that are used at least 1% of the time in the dataset.\n",
    "\n",
    "These matrices are saved as X_top1Percent_train_raw and X_top1Percent_test_raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83219a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create a list of single and double characters from the top 1% of to be used as features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a4482",
   "metadata": {},
   "source": [
    "The train_lang_dict() function takes in the raw vectorized X_train and y_train data sets and converts them into readable dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14148ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Command from Kaggle connects the character features to their specific language\n",
    "\n",
    "# Aggregate Unigrams per language\n",
    "def train_lang_dict(X_raw_counts, y_train):\n",
    "    lang_dict = {}\n",
    "    for i in range(len(y_train)):\n",
    "        lang = y_train[i]\n",
    "        v = np.array(X_raw_counts[i])\n",
    "        if not lang in lang_dict:\n",
    "            lang_dict[lang] = v\n",
    "        else:\n",
    "            lang_dict[lang] += v\n",
    "            \n",
    "    # to relative\n",
    "    for lang in lang_dict:\n",
    "        v = lang_dict[lang]\n",
    "        lang_dict[lang] = v / np.sum(v)\n",
    "        \n",
    "    return lang_dict\n",
    "\n",
    "# Collect relevant chars per language to Display them\n",
    "def getRelevantCharsPerLanguage(features, language_dict, significance=1e-5):\n",
    "    relevantCharsPerLanguage = {}\n",
    "    for lang in languages:\n",
    "        chars = []\n",
    "        relevantCharsPerLanguage[lang] = chars\n",
    "        v = language_dict[lang]\n",
    "        for i in range(len(v)):\n",
    "            if v[i] > significance:\n",
    "                chars.append(features[i])\n",
    "    return relevantCharsPerLanguage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb378224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giovanni/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# This will create a list of single and double characters from the top 1% of to be used as features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
    "\n",
    "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
    "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)\n",
    "\n",
    "language_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n",
    "\n",
    "top1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e18e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All items: 1100\n",
      "Unique items: 564\n"
     ]
    }
   ],
   "source": [
    "def getRelevantGramsPerLanguage(features, language_dict, top=50):\n",
    "    relevantGramsPerLanguage = {}\n",
    "    for lang in languages:\n",
    "        chars = []\n",
    "        relevantGramsPerLanguage[lang] = chars\n",
    "        v = language_dict[lang]\n",
    "        sortIndex = (-v).argsort()[:top]\n",
    "        for i in range(len(sortIndex)):\n",
    "            chars.append(features[sortIndex[i]])\n",
    "    return relevantGramsPerLanguage\n",
    "\n",
    "top50PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent)\n",
    "\n",
    "# top50\n",
    "allTop50 = []\n",
    "for lang in top50PerLanguage_dict:\n",
    "    allTop50 += set(top50PerLanguage_dict[lang])\n",
    "\n",
    "top50 = list(set(allTop50))\n",
    "    \n",
    "print('All items:', len(allTop50))\n",
    "print('Unique items:', len(top50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564dfaa",
   "metadata": {},
   "source": [
    "With this, we have 564 unique features which we can start training a model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fa4bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = pd.DataFrame(top50); data_trans.columns =[\"Chars\"] # convert data to data matrix\n",
    "\n",
    "data_trans = data_trans.transpose() # Take transpose of the data matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b05f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these arrays into dictonaries\n",
    "english_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "estonian_alpha = [A, B, D, E, F, G, H, I, J, K, L, M, N, O, P, R, S, Š, Z, Ž, T, U, V, Õ, Ä, Ö, Ü]\n",
    "swedish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, å, ä, ö]\n",
    "thai_alpha = [ก, ข, ค, ฅ, ฆ, ง, จ, ฉ, ช, ฌ, ญ, ฎ, ฏ, ฐ, ฑ, ฒ, ณ, ด, ต, ถ, ท, ธ, น, บ, บ, ผ, ฝ, พ, ฟ, ภ, \n",
    "               ม, ย, ร, ล, ว, ศ, ษ, ส, ห, ฬ, อ, ฮ] \n",
    "tamil_alpha = [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ங, ச, ஞ, ட, ண, த, ந, ன, ப, ம, ய, ர, ற, ல, ள, ழ, வ]\n",
    "dutch_alpha = english_alpha\n",
    "japanese_alpha = [ぁ, あ, ぃ, い, ぅ, う, ぇ, え, ぉ, お, か, が, き, ぎ, く, ぐ, け, げ, こ, ご, さ, ざ, し, じ, す, ず,\n",
    "                  せ, ぜ, そ, ぞ, た, だ, ち, ぢ, っ, つ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ,\n",
    "                  ふ, ぶ, ぷ, へ, べ, れ, る, り, ら, よ, ょ, ゆ, ゅ, や, ゃ, も, め, む, み, ま, ぽ, ぼ, ほ, ぺ, ろ, ゎ,\n",
    "                  わ, ゐ, ゑ, を, ん, ゔ, ゕ, ゖ,  ゚, ゛, ゜, ゝ, ゞ, ゟ, ゠, ァ, ア, サ, ゴ, コ, ゲ, ケ, グ, ク, ギ, キ,\n",
    "                  ガ, カ, オ, ォ, エ, ェ, ウ, ゥ, イ, ィ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ ,ダ ,チ ,ヂ, ッ, ツ, ヅ,\n",
    "                  テ, デ, ト, ホ, ペ, ベ, ヘ, プ, ブ, フ, ピ, ビ, ヒ, パ, バ, ハ, ノ, ネ, ヌ, ニ, ナ, ド, ボ, ポ, マ, ミ, \n",
    "                  ム, メ, モ, ャ, ヤ, ュ, ユ, ョ, ヨ, ラ, リ, ル, レ, ロ, ヮ, ㍿, ㍐, ヿ, ヾ, ヽ, ー, ・, ヺ, ヹ, ヸ, ヷ,\n",
    "                  ヶ, ヵ, ヴ, ン, ヲ, ヱ, ヰ, ワ]\n",
    "turkish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, r, s, t, u, v, y, z, ç, ğ, ı, İ, î, ö, ş, ü]\n",
    "latin_alpha = english_alpha\n",
    "urdu_alpha = [ش,س,ژ,ز,ڑ,ر,ذ,ڈ,د,خ,ح,چ,\n",
    "              ج,ث,ٹ,ت,پ,ب,آ,ا,ے,ی,ھ,ہ,و,ں,ن,م,ل,گ,ک,ق,ف,غ,ع,ظ,ط,ض,ص]\n",
    "indonesian_alpha = english_alpha\n",
    "portuguese_alpha = [ç, á, é, í, ó, ú, â, ê, ô, ã, õ, à, è, ì, ò, ù, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "french_alpha = [ç, é, â, ê, î, ô, û, à, è, ì, ò, ù, ë, ï, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "chinese_alpha = [胡, 赛, 尼, 本, 人, 和, 小, 说, 的, 主, 人, 公, 阿, 米, 尔, 一, 样, 都, 是, 出, 生, 在, 阿, 富, 汗, 首, 都, \n",
    "                 喀, 布, 尔, 少, 年, 时, 代, 便, 离, 开, 了, 这, 个, 国, 家, 。, 胡, 赛, 尼, 直, 到, 年, 小, 说, 出, 版, 之, \n",
    "                 后, 才, 首, 次, 回, 到, 已, 经, 离, 开, 年, 的, 祖, 国, 。, 他, 在, 苏, 联, 入, 侵, 时, 离, 开, 了, 阿, 富, \n",
    "                 汗, 而, 他, 的, 很, 多, 童, 年, 好, 友, 在, 阿, 富, 汗, 生, 活, 在, 他, 们, 出, 发, 之, 前, 罗, 伯, 特, 伊,\n",
    "                 达, 尔, 文, 卷, 查, 尔, 斯, 赖, 尔, 所, 著, 地, 质, 学, 原, 理, 在, 南, 美, 他, 得, 到, 第, 卷, 该, 书, 将, \n",
    "                 地, 形, 地, 貌, 解, 释, 为, 漫, 长, 历, 史, 时, 间, 渐, 进, 演, 变, 的, 的, 结, 果, 当, 他, 旅, 程, 的, 第, \n",
    "                 站, 抵, 达, 圣, 地, 亚, 哥, 佛, 得, 角, 的, 时, 候, 达, 尔, 文]\n",
    "korean_alpha = [ᄁ,ᄂ,ᄃ,ᄄ,ᄅᄆᄇ,ᄈ,ᄉ,ᄊ,ᄋ,ᄌᄍ,ᄎ,ᄏ,ᄐ,ᄑᄒ,아,악,안,알,암,압,앙,앞애,액,앵야,얀,약,양,얘,어,억,\n",
    "                언,얼,엄,업,엉,에,여,역,연,열,염,엽,영,예,ᄀ,여,역,연,열,염,엽,영,예,오,옥,온,올,옴,옹,와,완,왈,왕,왜,외,왼,\n",
    "                요,욕,용,우,욱,운,울,움,웅,워,원,월,위,유,육,윤,율,융,윷,으,은,을,음읍,응,의,이,익,인,일,임,입,잉,잎]\n",
    "hindi_alpha = [ऄ, अ, आ, इ, ई, उ, ऊ, ऋ, ऌ, ऍ, ऎ, ए, ऐ, ऑ, ऒ, ओ, औ, क, ख, ग, घ, ङ, च, छ, ज, झ, प, ऩ, न, ध, द, \n",
    "               थ, त, ण, ढ, ड, ठ, ट, ञ, फ, ब, भ, म, य, र, ऱ, ल, ळ, ऴ, व, श, ष, ४, ३, २, १, ०, ॥, ।, ॡ, ॠ, ॐ, ऽ, \n",
    "               ह, स, ५, ६, ७, ॲ, ॳ, ॴ, ॵ, ॶ, ॷ, ॹ, ॺ, ॻ, ॼ, ॾ, ॿ, ೱऀँं, ः, ऺ, ऻ, ा, ि, ी, ॎ, ॏॕैेॣॢ, ॗ]\n",
    "spanish_alpha = [á, é, í, ó, ú, ñ, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "pushto_alpha = [ﺏ ,پ ,ﺕ ,ټ ,ﺙ ,ﺝ ,چ ,ﺡ ,ﺥ ,څ ,ځ ,ﺩ ,ډ ,ﺫ ,ﺭ ,ړ ,ﺯ ,ژ ,ږ ,ﺱ ,ﺵ ,ښ ,ﺹ ,ﺽ ,ﻁ ,ﻅ ,ﻉ ,ﻍ ,ﻑ ,ﻕ ,ک ,ګ ,ﻝ ,ﻡ ,ﻥ ,ڼ, ,ﻭ ,ه ,ۀ ,ي ,ې ,ی ,ۍ ,ئ]\n",
    "persian_alpha = [,ش,س,ژ,ز,ر,ذ,د,خ,ح,چ,ج,ث,ت,پ,ب,آ,ا,ص,ض,ط,ظ,ع,غ,ف,ق,ک,گ,ل,م,ن,و,ه,ی]\n",
    "romanian_alpha = [ă, â, î, ș, ş, ț, ţ, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "russian_alpha = [б, в, г, д, ж, з, к, л, м, н, п, р, с, т, ф, х, ц, ч, ш, щ, а, е, ё, и, о, у, ы, э, ю, я, й]\n",
    "arabic_alpha = [ش,س,ز,ر,ذ,د,خ,ح,ج,ث,ت,ب,ا,ء,ي,و,ه,ن,م,ل,ك,ق,ف,غ,ع,ظ,ط,ض,ص]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters for first model\n",
    "chars = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
