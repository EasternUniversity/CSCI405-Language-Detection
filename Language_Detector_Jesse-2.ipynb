{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dd55be",
   "metadata": {},
   "source": [
    "# Understanding the Data\n",
    "The dataset acquired from [Kaggle](https://www.kaggle.com/code/martinkk5575/language-detection/data) contains words from several different languages. The noise contained in the dataset are duplicate words. To reduce this noise, the words will be broken down into single and double characters, then rated based on how often they show up in that respective language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b50dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21995</th>\n",
       "      <td>hors du terrain les années  et  sont des année...</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21996</th>\n",
       "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21997</th>\n",
       "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21998</th>\n",
       "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21999</th>\n",
       "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
       "      <td>Romanian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
       "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
       "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
       "...                                                  ...       ...\n",
       "21995  hors du terrain les années  et  sont des année...    French\n",
       "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
       "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
       "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
       "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
       "\n",
       "[22000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import data*\n",
    "fileName = \"dataset.csv\"\n",
    "data = pd.read_csv(fileName)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63422d",
   "metadata": {},
   "source": [
    "# Splitting the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18753df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data['Text'] # Feature matrix\n",
    "y=data['language'] # Label\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the languages into a DataFrame that we aren't modifying\n",
    "languages = set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b57a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cow = pd.DataFrame()\n",
    "print(type(cow))\n",
    "print(type(cow)==pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02b1ea",
   "metadata": {},
   "source": [
    "# Creating Functions to Feature Engineer Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "302dea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(dataframe, chars):\n",
    "    new_arr = np.zeros((1, len(chars)))\n",
    "    j=0\n",
    "    for char in chars:\n",
    "        count = 0.0\n",
    "        for letter in dataframe:\n",
    "            if letter == char:\n",
    "                count = count + 1.0\n",
    "            fraction = count/len(dataframe)\n",
    "        new_arr[0,j] = fraction\n",
    "        j = j+1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "    return data_frame\n",
    "\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_trial(dataframe, chars):\n",
    "    if (type(type(dataframe)==str)):\n",
    "        new_arr = np.zeros((1, len(chars)))\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in dataframe:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(dataframe)\n",
    "            new_arr[0,j] = fraction\n",
    "            j = j+1\n",
    "        data_frame = pd.DataFrame(new_arr, columns = chars)\n",
    "        return data_frame\n",
    "        \n",
    "    if (type(type(dataframe)==pd.DataFrame)):\n",
    "        arr = dataframe.to_numpy()\n",
    "        new_arr = np.zeros((len(arr), len(chars)))\n",
    "        i=0\n",
    "        j=0\n",
    "        for text in arr:\n",
    "            sentence = text\n",
    "            count = 0.0\n",
    "            j=0\n",
    "            for char in chars:\n",
    "                count = 0.0\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "                    fraction = count/len(sentence)\n",
    "                new_arr[i,j] = fraction\n",
    "                j = j + 1\n",
    "        \n",
    "            i = i + 1\n",
    "        data_frame = pd.DataFrame(new_arr, columns = chars)      \n",
    "        return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864880d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j=0\n",
    "        for char in chars:\n",
    "            count = 0.0\n",
    "            for letter in sentence:\n",
    "                if letter == char:\n",
    "                    count = count + 1.0\n",
    "                fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j + 1\n",
    "        \n",
    "        i = i + 1\n",
    "    data_frame = pd.DataFrame(new_arr, columns = chars)      \n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30accace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_2(dataframe, chars):\n",
    "    arr = dataframe.to_numpy()\n",
    "    new_arr = np.zeros((len(arr), len(chars)))\n",
    "    i=0\n",
    "    j=0\n",
    "    for text in arr:\n",
    "        sentence = text\n",
    "        count = 0.0\n",
    "        j = 0\n",
    "        for list in chars:\n",
    "            count = 0.0\n",
    "            for char in list:\n",
    "                for letter in sentence:\n",
    "                    if letter == char:\n",
    "                        count = count + 1.0\n",
    "            fraction = count/len(sentence)\n",
    "            new_arr[i,j] = fraction\n",
    "            j = j+1\n",
    "        i = i+1\n",
    "    \n",
    "    names = ['english', 'estonian', 'swedish', 'thai', 'tamil', 'dutch', 'japanese', 'turkish', 'latin', 'urdu',\n",
    "             'indonesian', 'portuguese', 'french', 'chinese', 'korean', 'hindi', 'spanish', 'pushto', 'persian',\n",
    "             'romanian', 'russian', 'arabic']\n",
    "    \n",
    "    data_frame = pd.DataFrame(new_arr, columns = names)\n",
    "    return data_frame\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0332362",
   "metadata": {},
   "source": [
    "# Testing Our Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41028870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>estonian</th>\n",
       "      <th>swedish</th>\n",
       "      <th>thai</th>\n",
       "      <th>tamil</th>\n",
       "      <th>dutch</th>\n",
       "      <th>japanese</th>\n",
       "      <th>turkish</th>\n",
       "      <th>latin</th>\n",
       "      <th>urdu</th>\n",
       "      <th>...</th>\n",
       "      <th>french</th>\n",
       "      <th>chinese</th>\n",
       "      <th>korean</th>\n",
       "      <th>hindi</th>\n",
       "      <th>spanish</th>\n",
       "      <th>pushto</th>\n",
       "      <th>persian</th>\n",
       "      <th>romanian</th>\n",
       "      <th>russian</th>\n",
       "      <th>arabic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.263780</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192913</td>\n",
       "      <td>0.598425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.031142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    english  estonian   swedish      thai     tamil     dutch  japanese  \\\n",
       "0  0.000000  0.000000  0.000000  0.118919  0.000000  0.000000       0.0   \n",
       "1  0.000000  0.000000  0.000000  0.157277  0.000000  0.000000       0.0   \n",
       "2  0.523622  0.263780  0.523622  0.000000  0.000000  0.523622       0.0   \n",
       "3  0.038062  0.013841  0.031142  0.000000  0.034602  0.038062       0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000       0.0   \n",
       "\n",
       "    turkish     latin  urdu  ...  french  chinese  korean  hindi   spanish  \\\n",
       "0  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "1  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "2  0.192913  0.598425   0.0  ...     0.0      0.0     0.0    0.0  0.208661   \n",
       "3  0.006920  0.031142   0.0  ...     0.0      0.0     0.0    0.0  0.013841   \n",
       "4  0.000000  0.000000   0.0  ...     0.0      0.0     0.0    0.0  0.000000   \n",
       "\n",
       "   pushto  persian  romanian   russian  arabic  \n",
       "0     0.0      0.0       0.0  0.000000     0.0  \n",
       "1     0.0      0.0       0.0  0.000000     0.0  \n",
       "2     0.0      0.0       0.0  0.000000     0.0  \n",
       "3     0.0      0.0       0.0  0.000000     0.0  \n",
       "4     0.0      0.0       0.0  0.264471     0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]\n",
    "\n",
    "panda = X_train.head()\n",
    "some_data = feature_engineering_2(panda, new_chars)\n",
    "some_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62084333",
   "metadata": {},
   "source": [
    "# 1st Model: RandomForrestClassifier\n",
    "\n",
    "This model is trained on a list of 44 characters and the first 3000 rows of our X_train data. Our performance metric is accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e0dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ready = feature_engineering(X_train.head(9000), chars_2)\n",
    "y_train_ready = y_train.head(9000)\n",
    "X_test_ready = feature_engineering(X_test, chars_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce351065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8822727272727273\n"
     ]
    }
   ],
   "source": [
    "#First Model Trained\n",
    "#characters for first model\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "X_train_2 = X_train.head(3000)\n",
    "model_one = feature_engineering(X_train_2, chars_2)\n",
    "model_one\n",
    "\n",
    "y_train_one = y_train.head(3000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf.fit(model_one, y_train_one)\n",
    "\n",
    "X_test_1 = feature_engineering(X_test, chars_2)\n",
    "\n",
    "y_preds = rnd_clf.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))\n",
    "\n",
    "import pickle #ask about pickle\n",
    "\n",
    "saved_model = pickle.dumps(rnd_clf)\n",
    "\n",
    "rdf_from_pickle = pickle.loads(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcf78c7",
   "metadata": {},
   "source": [
    "Same model, but training on the whole X_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cef6e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8809090909090909\n"
     ]
    }
   ],
   "source": [
    "#First Model Trained\n",
    "#characters for first model\n",
    "chars_2 = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "X_train_ready = feature_engineering(X_train, chars_2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf_full = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf_full.fit(X_train_ready, y_train)\n",
    "\n",
    "X_test_ready = feature_engineering(X_test, chars_2)\n",
    "\n",
    "y_predictions = rnd_clf_full.predict(X_test_ready)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_predictions)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994469f4",
   "metadata": {},
   "source": [
    "# Predicting the language of a string\n",
    "\n",
    "Using the predict method to send a string and predict what language it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40deba5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Swedish'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Hej, jag heter Jesse Byler. Den här meningen är på engelska, så förhoppningsvis klassas den som engelska. Det skulle vara trevligt, eller hur?\"\n",
    "hello = test_function(string, chars_2)\n",
    "pred_swed = rnd_clf.predict(hello)\n",
    "pred_swed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a9cd4",
   "metadata": {},
   "source": [
    "# Second Model: RandomForrestClassifier\n",
    "BUT: we will send a list of lists into our feature_engeering function instead of a list of characters! We will train the model on the first 6,000 rows of the X_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2238993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8252272727272727\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = X_train.head(6000)\n",
    "X_train_2_eng = feature_engineering_2(X_train_2, new_chars)\n",
    "\n",
    "y_train_2 = y_train.head(6000)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf_2 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
    "rnd_clf_2.fit(X_train_2_eng, y_train_2)\n",
    "\n",
    "X_test_2 = feature_engineering_2(X_test, new_chars)\n",
    "y_preds_2 = rnd_clf_2.predict(X_test_2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score_2 = accuracy_score(y_test, y_preds_2)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad263f",
   "metadata": {},
   "source": [
    "# Third Model: Decision Tree Model\n",
    "Now I will train a decision tree model, later a grid search will be conducted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f3134f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2e623e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8811363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "preds = tree_clf.predict(feature_engineering(X_test, chars_2))\n",
    "\n",
    "acc_score_tree = accuracy_score(y_test, preds)\n",
    "print('Accuracy=%s' % (acc_score_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da09418",
   "metadata": {},
   "source": [
    "# Grid Searches for Decision Tree Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e61412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n",
      "The best parameters are:  {'max_depth': 20, 'max_leaf_nodes': 50, 'min_samples_split': 40}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [2,4,6,8,10,20,30,40,50], 'max_leaf_nodes': [2,4,6,8,10,20,30,40,50], \n",
    "                          'min_samples_split': [2,4,6,8,10,20,30,40,50]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                              param_grid,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "grid_search_cv.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f4d6a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 343 candidates, totalling 1029 fits\n",
      "The best parameters are:  {'max_depth': 21, 'max_leaf_nodes': 70, 'min_samples_split': 39}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [15,17,19,20,21,23,25], 'max_leaf_nodes': [40,45,50,55,60,70,80], \n",
    "                          'min_samples_split': [35,37,39,40,41,43,45]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                              param_grid,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "grid_search_cv.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3d783e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 63 candidates, totalling 189 fits\n",
      "The best parameters are:  {'max_depth': 22, 'max_leaf_nodes': 75, 'min_samples_split': 38}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [20,21,22], 'max_leaf_nodes': [65,67,69,70,71,73,75], \n",
    "                          'min_samples_split': [38,39,40]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                              param_grid,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "grid_search_cv.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3ecf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 175 candidates, totalling 525 fits\n",
      "The best parameters are:  {'max_depth': 22, 'max_leaf_nodes': 75, 'min_samples_split': 36}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [20,21,22,23,24], 'max_leaf_nodes': [73,74,75,76,77,78,79], \n",
    "                          'min_samples_split': [36,37,38,39,40]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
    "                              param_grid,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "grid_search_cv.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993af22",
   "metadata": {},
   "source": [
    "# Fourth Model: Decision Tree with grid searched parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef49c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8934090909090909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf_opt = DecisionTreeClassifier(max_depth=22, max_leaf_nodes=75,\n",
    "                                      min_samples_split=36, random_state=42)\n",
    "tree_clf_opt.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "opt_preds = tree_clf_opt.predict(feature_engineering(X_test, chars_2))\n",
    "\n",
    "acc_score_tree_opt = accuracy_score(y_test, opt_preds)\n",
    "print('Accuracy=%s' % (acc_score_tree_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c93b4",
   "metadata": {},
   "source": [
    "With the optimal hyperparameters, the accuracy score went up 1.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153e9ca",
   "metadata": {},
   "source": [
    "# Grid Searches for Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18b9386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 360 candidates, totalling 1080 fits\n",
      "The best parameters are:  {'max_depth': 40, 'min_samples_split': 10, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_2 = {'max_depth': [1,2,3,4,5,10,20,30,40,50], 'n_estimators': [10,100,200,300,400,500],\n",
    "              'min_samples_split': [5,10,20,30,40,50]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf477399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 147 candidates, totalling 441 fits\n",
      "The best parameters are:  {'max_depth': 45, 'min_samples_split': 7, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [35,37,39,40,41,43,45], 'n_estimators': [500,600,700],\n",
    "              'min_samples_split': [5,7,9,10,11,13,15]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "494da484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 140 candidates, totalling 420 fits\n",
      "The best parameters are:  {'max_depth': 45, 'min_samples_split': 7, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [40,42,44,45,46,48,50], 'n_estimators': [160,600,700,800],\n",
    "              'min_samples_split': [5,6,7,8,9]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b3a9061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "The best parameters are:  {'max_depth': 45, 'min_samples_split': 7, 'n_estimators': 700}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [45], 'n_estimators': [650,675,700,725,750],\n",
    "              'min_samples_split': [7]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3527ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "The best parameters are:  {'max_depth': 45, 'min_samples_split': 7, 'n_estimators': 715}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [45], 'n_estimators': [685,695,700,705,715],\n",
    "              'min_samples_split': [7]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_2 = {'max_depth': [45], 'n_estimators': [710,714,715,716,720],\n",
    "              'min_samples_split': [7]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d25515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "The best parameters are:  {'max_depth': 45, 'min_samples_split': 7, 'n_estimators': 716}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [45], 'n_estimators': [715,716,717,718],\n",
    "              'min_samples_split': [7]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21200332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "The best parameters are:  {'max_depth': 45, 'max_leaf_nodes': 50, 'min_samples_split': 7, 'n_estimators': 716}\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {'max_depth': [45], 'n_estimators': [716],\n",
    "              'min_samples_split': [7], 'max_leaf_nodes': [10,20,30,40,50]}\n",
    "\n",
    "grid_search_cv_2 = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                              param_grid_2,\n",
    "                              verbose=1,\n",
    "                              cv=3)\n",
    "\n",
    "grid_search_cv_2.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "print(\"The best parameters are: \", grid_search_cv_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81b7f2",
   "metadata": {},
   "source": [
    "# Fifth Model: Random Forrest with Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65aebd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9113636363636364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf_opt = RandomForestClassifier(n_estimators=716, min_samples_split=7, max_depth=45, max_leaf_nodes=50, random_state=42)\n",
    "rnd_clf_opt.fit(X_train_ready, y_train_ready)\n",
    "\n",
    "y_preds_opt = rnd_clf_opt.predict(X_test_ready)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(y_test, y_preds_opt)\n",
    "\n",
    "print('Accuracy=%s' % (acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943f4cf",
   "metadata": {},
   "source": [
    "# Sixth Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa31f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ready = feature_engineering(X_train.head(9000), chars_2)\n",
    "y_train_ready = y_train.head(9000)\n",
    "X_test_ready = feature_engineering(X_test, chars_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f857adad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.4152272727272727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(feature_engineering(X_train.head(9000), chars_2), y_train.head(9000))\n",
    "y_predict = NB_clf.predict(feature_engineering(X_test, chars_2))\n",
    "\n",
    "acc_score_NB = accuracy_score(y_test, y_predict)\n",
    "print('Accuracy=%s' % (acc_score_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9bb4e",
   "metadata": {},
   "source": [
    "# Creating lists of alphabets for languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b05f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create these arrays into dictonaries\n",
    "english_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "estonian_alpha = [A, B, D, E, F, G, H, I, J, K, L, M, N, O, P, R, S, Š, Z, Ž, T, U, V, Õ, Ä, Ö, Ü]\n",
    "swedish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, å, ä, ö]\n",
    "thai_alpha = [ก, ข, ค, ฅ, ฆ, ง, จ, ฉ, ช, ฌ, ญ, ฎ, ฏ, ฐ, ฑ, ฒ, ณ, ด, ต, ถ, ท, ธ, น, บ, บ, ผ, ฝ, พ, ฟ, ภ, \n",
    "               ม, ย, ร, ล, ว, ศ, ษ, ส, ห, ฬ, อ, ฮ] \n",
    "tamil_alpha = [அ, ஆ, இ, ஈ, உ, ஊ, எ, ஏ, ஐ, ஒ, ஓ, ஔ, க, ங, ச, ஞ, ட, ண, த, ந, ன, ப, ம, ய, ர, ற, ல, ள, ழ, வ]\n",
    "dutch_alpha = english_alpha\n",
    "japanese_alpha = [ぁ, あ, ぃ, い, ぅ, う, ぇ, え, ぉ, お, か, が, き, ぎ, く, ぐ, け, げ, こ, ご, さ, ざ, し, じ, す, ず,\n",
    "                  せ, ぜ, そ, ぞ, た, だ, ち, ぢ, っ, つ, づ, て, で, と, ど, な, に, ぬ, ね, の, は, ば, ぱ, ひ, び, ぴ,\n",
    "                  ふ, ぶ, ぷ, へ, べ, れ, る, り, ら, よ, ょ, ゆ, ゅ, や, ゃ, も, め, む, み, ま, ぽ, ぼ, ほ, ぺ, ろ, ゎ,\n",
    "                  わ, ゐ, ゑ, を, ん, ゔ, ゕ, ゖ,  ゚, ゛, ゜, ゝ, ゞ, ゟ, ゠, ァ, ア, サ, ゴ, コ, ゲ, ケ, グ, ク, ギ, キ,\n",
    "                  ガ, カ, オ, ォ, エ, ェ, ウ, ゥ, イ, ィ, ザ, シ, ジ, ス, ズ, セ, ゼ, ソ, ゾ, タ ,ダ ,チ ,ヂ, ッ, ツ, ヅ,\n",
    "                  テ, デ, ト, ホ, ペ, ベ, ヘ, プ, ブ, フ, ピ, ビ, ヒ, パ, バ, ハ, ノ, ネ, ヌ, ニ, ナ, ド, ボ, ポ, マ, ミ, \n",
    "                  ム, メ, モ, ャ, ヤ, ュ, ユ, ョ, ヨ, ラ, リ, ル, レ, ロ, ヮ, ㍿, ㍐, ヿ, ヾ, ヽ, ー, ・, ヺ, ヹ, ヸ, ヷ,\n",
    "                  ヶ, ヵ, ヴ, ン, ヲ, ヱ, ヰ, ワ]\n",
    "turkish_alpha = [a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, r, s, t, u, v, y, z, ç, ğ, ı, İ, î, ö, ş, ü]\n",
    "latin_alpha = english_alpha\n",
    "urdu_alpha = [ش,س,ژ,ز,ڑ,ر,ذ,ڈ,د,خ,ح,چ,\n",
    "              ج,ث,ٹ,ت,پ,ب,آ,ا,ے,ی,ھ,ہ,و,ں,ن,م,ل,گ,ک,ق,ف,غ,ع,ظ,ط,ض,ص]\n",
    "indonesian_alpha = english_alpha\n",
    "portuguese_alpha = [ç, á, é, í, ó, ú, â, ê, ô, ã, õ, à, è, ì, ò, ù, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "french_alpha = [ç, é, â, ê, î, ô, û, à, è, ì, ò, ù, ë, ï, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "chinese_alpha = [胡, 赛, 尼, 本, 人, 和, 小, 说, 的, 主, 人, 公, 阿, 米, 尔, 一, 样, 都, 是, 出, 生, 在, 阿, 富, 汗, 首, 都, \n",
    "                 喀, 布, 尔, 少, 年, 时, 代, 便, 离, 开, 了, 这, 个, 国, 家, 。, 胡, 赛, 尼, 直, 到, 年, 小, 说, 出, 版, 之, \n",
    "                 后, 才, 首, 次, 回, 到, 已, 经, 离, 开, 年, 的, 祖, 国, 。, 他, 在, 苏, 联, 入, 侵, 时, 离, 开, 了, 阿, 富, \n",
    "                 汗, 而, 他, 的, 很, 多, 童, 年, 好, 友, 在, 阿, 富, 汗, 生, 活, 在, 他, 们, 出, 发, 之, 前, 罗, 伯, 特, 伊,\n",
    "                 达, 尔, 文, 卷, 查, 尔, 斯, 赖, 尔, 所, 著, 地, 质, 学, 原, 理, 在, 南, 美, 他, 得, 到, 第, 卷, 该, 书, 将, \n",
    "                 地, 形, 地, 貌, 解, 释, 为, 漫, 长, 历, 史, 时, 间, 渐, 进, 演, 变, 的, 的, 结, 果, 当, 他, 旅, 程, 的, 第, \n",
    "                 站, 抵, 达, 圣, 地, 亚, 哥, 佛, 得, 角, 的, 时, 候, 达, 尔, 文]\n",
    "korean_alpha = [ᄁ,ᄂ,ᄃ,ᄄ,ᄅᄆᄇ,ᄈ,ᄉ,ᄊ,ᄋ,ᄌᄍ,ᄎ,ᄏ,ᄐ,ᄑᄒ,아,악,안,알,암,압,앙,앞애,액,앵야,얀,약,양,얘,어,억,\n",
    "                언,얼,엄,업,엉,에,여,역,연,열,염,엽,영,예,ᄀ,여,역,연,열,염,엽,영,예,오,옥,온,올,옴,옹,와,완,왈,왕,왜,외,왼,\n",
    "                요,욕,용,우,욱,운,울,움,웅,워,원,월,위,유,육,윤,율,융,윷,으,은,을,음읍,응,의,이,익,인,일,임,입,잉,잎]\n",
    "hindi_alpha = [ऄ, अ, आ, इ, ई, उ, ऊ, ऋ, ऌ, ऍ, ऎ, ए, ऐ, ऑ, ऒ, ओ, औ, क, ख, ग, घ, ङ, च, छ, ज, झ, प, ऩ, न, ध, द, \n",
    "               थ, त, ण, ढ, ड, ठ, ट, ञ, फ, ब, भ, म, य, र, ऱ, ल, ळ, ऴ, व, श, ष, ४, ३, २, १, ०, ॥, ।, ॡ, ॠ, ॐ, ऽ, \n",
    "               ह, स, ५, ६, ७, ॲ, ॳ, ॴ, ॵ, ॶ, ॷ, ॹ, ॺ, ॻ, ॼ, ॾ, ॿ, ೱऀँं, ः, ऺ, ऻ, ा, ि, ी, ॎ, ॏॕैेॣॢ, ॗ]\n",
    "spanish_alpha = [á, é, í, ó, ú, ñ, ü, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "pushto_alpha = [ﺏ ,پ ,ﺕ ,ټ ,ﺙ ,ﺝ ,چ ,ﺡ ,ﺥ ,څ ,ځ ,ﺩ ,ډ ,ﺫ ,ﺭ ,ړ ,ﺯ ,ژ ,ږ ,ﺱ ,ﺵ ,ښ ,ﺹ ,ﺽ ,ﻁ ,ﻅ ,ﻉ ,ﻍ ,ﻑ ,ﻕ ,ک ,ګ ,ﻝ ,ﻡ ,ﻥ ,ڼ, ,ﻭ ,ه ,ۀ ,ي ,ې ,ی ,ۍ ,ئ]\n",
    "persian_alpha = [,ش,س,ژ,ز,ر,ذ,د,خ,ح,چ,ج,ث,ت,پ,ب,آ,ا,ص,ض,ط,ظ,ع,غ,ف,ق,ک,گ,ل,م,ن,و,ه,ی]\n",
    "romanian_alpha = [ă, â, î, ș, ş, ț, ţ, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z]\n",
    "russian_alpha = [б, в, г, д, ж, з, к, л, м, н, п, р, с, т, ф, х, ц, ч, ш, щ, а, е, ё, и, о, у, ы, э, ю, я, й]\n",
    "arabic_alpha = [ش,س,ز,ر,ذ,د,خ,ح,ج,ث,ت,ب,ا,ء,ي,و,ه,ن,م,ل,ك,ق,ف,غ,ع,ظ,ط,ض,ص]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters for first model\n",
    "chars = ['e', 't', 'ä', 'ö', 'a', 'n', 'ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', \n",
    "         'o', 'r', 'ー', '日', 'あ', 'ぁ', 'ぇ', 'ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'i', 'u', 'چ', 'ح', 'خ', 'ش',\n",
    "         'â', 'ù', 'è', 's', 'î', 'ë', '胡', '童', '。', 'ᄁ', '알', '에', 'ᄃ', 'ऺ', 'त', 'ऻ', 'क', 'á', 'é', 'í', \n",
    "         'ó', 'ږ','ک', 'ﻑ', 'ی', 'م', 'ث', 'ţ', 'ă', 'ș', 'ş', 'б', 'в', 'г', 'д', 'ص', 'ف', 'ج', 'ر']\n",
    "\n",
    "new_chars = [['e', 't', 'a', 'i', 'o', 'n', 's', 'h', 'r'], ['a', 'e', 'i', 'ä', 'ö', 'õ', 'š', 'ü', 'ž'], \n",
    "             ['å', 'ä', 'ö', 'a', 'e', 't', 'n', 'r', 's', 'i'], ['ก', 'ข', 'ค', 'ฅ', 'ฆ', 'ง', 'จ', 'ฉ', 'ช', 'ฌ'],\n",
    "             ['அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ'], ['a', 'e', 'i', 'o', 'h', 'n', 'r', 't', 's'], \n",
    "             ['㍿', '㍐', 'ヿ', 'ヾ', 'ヽ', 'ー', '・', 'ヺ', 'ヹ', 'ヸ'], ['ç', 'ğ', 'ı', 'İ', 'î', 'ö', 'ş', 'ü', 'a', 'e'],\n",
    "             ['a', 'e', 'i', 'n', 'r', 's', 't', 'u', 'm', 'd'], ['چ', 'ح', 'خ', 'ش', 'ن', 'ٹ', 'ن', 'ث', 'گ', 'ج'],\n",
    "             ['a', 'A', 'i', 'n', 'r', 'm', 's', 't', 'u', 'g'], ['â', 'ê', 'ô', 'ã', 'õ', 'à', 'è', 'ì', 'ò', 'ù'],\n",
    "             ['ô', 'û', 'à', 'è', 'ì', 'ò', 'ù', 'ë', 'ï', 'ü'], ['主', '人', '公', '阿', '米', '尔', '一', '样', '都', '是'],\n",
    "             ['응','의','이','익','인','일','임','입','잉','잎'], ['ः', 'ऺ', 'ऻ', 'ा', 'ि', 'ी', 'ॎ', 'ई', 'उ', 'ऊ'], \n",
    "             ['á', 'é', 'í', 'ó', 'ú', 'ñ', 'ü', 't', 'e', 'i'], ['ت', 'ا', 'ې', 'ښ', 'ن', 'ر', 'ع', 'ط', 'ړ', 'س'],\n",
    "             ['ق', ' غ', 'ج', 'ت', ' ن ', 'ی', 'ل ', 'ظ', 'ص', 'ز'], ['ă', 'â', 'î', 'ș', 'ş', 'ț', 'ţ'], \n",
    "             ['б', 'в', 'г', 'д', 'ж', 'з', 'к', 'л', 'м', 'н'], ['م', 'ص', 'ظ', 'و', 'ر', 'م', 'ي', 'ج', 'ز', 'ق']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
